<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="google-site-verification" content="QMvy4LUd9kqEbkLgKUPNyuYlMu0E12nfHPE_xX4zg9I" />
    <script src="https://cdn.jsdelivr.net/npm/fuse.js/dist/fuse.js"></script>
    <title>Comparing differences in depthwise separable and normal convs in pytorch</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@700&family=Open+Sans&family=Lora&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/static/css/style.css">
    <link rel="stylesheet" href="/static/css/pygments.css">
    <!-- MathJax Configuration -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="/static/js/main.js" defer></script>
</head>
<body>
    <div class="container">
        <nav class="navbar">
            <div class="logo">MONOQ</div>
            <div class="hamburger">â˜°</div>
            <div class="nav-links">
                <a href="/">Home</a>
                <a href="/posts/">Posts</a>
                <a href="/highlights/">Highlights</a>
                <button id="theme-toggle">Toggle Theme</button>
            </div>
        </nav>
        
        <main>
            
<article>
    <h1>Comparing differences in depthwise separable and normal convs in pytorch</h1>
    <p>2025-03-09</p>
    <div><h4>Disclaimer:</h4>
<p>This is not a scientific paper or report or completely accurate in any way. It is simply a personal diary/note that I have made while trying to learn ML. </p>
<h3>Abstract:</h3>
<p>Convolutional neural networks (CNNs) have witnessed rapid evolution, with computational efficiency emerging as a key design criterion. This study compares standard convolutional operations with depthwise separable convolutions using the PyTorch framework. We derive the theoretical computational costs as follows: for standard convolutions, 
$$
\text{Cost} = K^2 \times C_{\text{in}} \times C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}}
$$</p>
<p>and for depthwise separable convolutions, 
$$ 
\text{Cost} = K^2 \times C_{\text{in}} \times H_{\text{out}} \times W_{\text{out}} + C_{\text{in}} \times C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}}
$$  </p>
<p>Using benchmark datasets such as CIFAR-10, our empirical evaluations explore the trade-offs between reduced computational load and potential impacts on model accuracy. Preliminary results indicate that depthwise separable convolutions can significantly lower computational demands with minimal loss in accuracy, offering promising insights for the design of efficient neural architectures in resource-constrained environments.</p></div>
</article>

        </main>
    </div>
</body>
</html>
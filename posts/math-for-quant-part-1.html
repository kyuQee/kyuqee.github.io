<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="google-site-verification" content="QMvy4LUd9kqEbkLgKUPNyuYlMu0E12nfHPE_xX4zg9I" />
    <title>Beyond High School Probability: Unlocking Binomial, Gaussian, and More</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@700&family=Open+Sans&family=Lora&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="/static/css/style.css">
    <link rel="stylesheet" href="/static/css/pygments.css">
    <!-- MathJax Configuration -->
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] },
            svg: { fontCache: 'global' }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="/static/js/main.js" defer></script>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0Z497HS9D6"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-0Z497HS9D6');
    </script>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Sans+JP&display=swap" rel="stylesheet">
</head>

<body>
    <div class="container">
        <nav class="navbar">
            <a href="/" class="logo">MONOQ</a>
            <div class="hamburger">â˜°</div>
            <div class="nav-content">
                <ul class="main-nav">
                    <li><a href="/">Home</a></li>
                    <li><a href="/posts/">Posts</a></li>
                    <li><a href="/highlights/">Highlights</a></li>
                    <li><a href="/about/">About Me</a></li>
                </ul>
                <div class="social-links">
                    <a href="https://github.com/kyuqee" target="_blank">
                        <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png"
                            alt="GitHub" class="social-icon invertible">
                    </a>
                    <a href="mailto:kyuqeg@gmail.com">
                        <img src="https://cdn-icons-png.flaticon.com/512/561/561127.png" alt="Email"
                            class="social-icon invertible">
                    </a>
                </div>
                <button id="theme-toggle">Toggle Theme</button>
            </div>
        </nav>
        <main>
            
<article>
    <h1>Beyond High School Probability: Unlocking Binomial, Gaussian, and More</h1>
    <p>2025-05-17</p>
    <br>

    
    <div class="toc-wrapper">
        <h2>Table of Contents</h2>
        <div class="toc">
            <div class="toc">
<ul>
<li><a href="#work-in-progress">WORK IN PROGRESS</a></li>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#resources">Resources</a></li>
<li><a href="#prerequisites-for-reading">Prerequisites for Reading</a></li>
<li><a href="#the-paradox">The Paradox</a><ul>
<li><a href="#method-1-random-midpoint-method">Method 1: Random Midpoint Method</a></li>
<li><a href="#method-2-diameter-method">Method 2: Diameter Method</a></li>
<li><a href="#method-3-radius-method">Method 3: Radius Method</a></li>
<li><a href="#the-contradiction">The Contradiction</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#terms-and-definitions">Terms and Definitions</a><ul>
<li><a href="#what-is-probability">What is Probability?</a></li>
<li><a href="#sample-space">Sample Space</a></li>
<li><a href="#sigma-algebra">Sigma Algebra</a><ul>
<li><a href="#vitali-set-deep-dive">Vitali Set (Deep Dive)</a></li>
</ul>
</li>
<li><a href="#the-borel-sigma-algebra">The Borel Sigma Algebra</a></li>
<li><a href="#probability-measures-and-probability-spaces">Probability Measures and Probability Spaces</a><ul>
<li><a href="#probability-measures">Probability Measures</a></li>
<li><a href="#probability-spaces">Probability Spaces</a></li>
</ul>
</li>
<li><a href="#random-variables">Random Variables</a><ul>
<li><a href="#discrete-random-variables">Discrete Random Variables</a></li>
<li><a href="#continous-random-variables">Continous Random Variables</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#probability-distributions">Probability distributions</a><ul>
<li><a href="#the-pmf-and-the-pdf">The PMF and the PDF</a><ul>
<li><a href="#probability-mass-function-pmf">Probability Mass Function (PMF)</a></li>
<li><a href="#probability-density-function-pdf">Probability Density Function (PDF)</a><ul>
<li><a href="#example-1-uniform-distribution-on-an-interval-1d">Example 1: Uniform Distribution on an Interval (1D)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#the-binomial-distribution">The Binomial distribution</a></li>
</ul>
</li>
</ul>
</div>

        </div>
    </div>
    
    <br>
    <div><h1 id="work-in-progress">WORK IN PROGRESS</h1>
<h1 id="introduction">Introduction</h1>
<p>High school curricula typically introduce basic probability concepts; however, probability theory applied in financial mathematics and statistics requires significantly advanced techniques. To enhance my understanding and proficiency, I will undertake an in-depth exploration of probability theory, encompassing distributions such as Binomial, Gaussian, and Poisson, as well as concepts including random variables, sigma algebras, and probability spaces.</p>
<p>This is by no means a proper text, or learning resource. This is simply what <strong>I would've wanted to read</strong>, when I was getting started with these topics. Here are the resources I used to learn these topics (still learning, but more less understood the basics).</p>
<h2 id="resources">Resources</h2>
<ol>
<li><strong>MIT OCW 18.S096</strong>: <a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP63ctJIEC1UnZ0btsphnnoHR">youtube link</a>, <a href="https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-with-applications-in-finance-fall-2013/">website link</a>. It's pretty good, but required a lot of previous undergrad knowledge so the learning curve was quite steep. It did however introduce me to the right topics to learn, and I learnt them all individually from different resources.</li>
<li><strong>Probability and Stochastics for finance, NPTEL</strong>: <a href="https://www.youtube.com/@probabilityandstochasticsf5278">youtube link</a>, Extremely good, especially since it starts right from the highschool level. This is also where I got the inspiration for the Bertrand Paradox, presented ahead.</li>
<li><strong>Quantitative Finance 101, FEBS IIT Bhubaneshwar</strong>: <a href="https://www.youtube.com/playlist?list=PLX5wiDP8rz62dRofitFcP10iqLTjLKfmq">youtube link</a>. This made me understand sigma algebras, atleast at a level where I can apply them reasonably. It doesn't really assume zero knowledge, but is pretty in depth.    </li>
<li><strong>Shreve Stochastic Calculus for Finance II: Continuous-Time Models</strong>: <a href="https://g.co/kgs/n3SQKnz">link</a>. This book was recommended multiple times, no matter where I looked on the internet. It is in-fact the go-to book for this topic. </li>
</ol>
<p>Apart from these I've used several other miscellaneous sources, which I won't state here for the sake of brevity.</p>
<h2 id="prerequisites-for-reading">Prerequisites for Reading</h2>
<p><strong>High School Math:</strong> A solid foundation in algebra, coordinate geometry, trigonometry, and basic probability (e.g., AP Calculus/Statistics, JEE Advanced, IB Math HL, or A-Level Mathematics). </p>
<p><strong>Set Notation:</strong> Comfort with basic set notation (e.g., describing points in a plane or intervals).    </p>
<p><strong>Geometric Intuition:</strong> Ability to visualize geometric shapes like circles and chords. </p>
<p><strong>Curiosity:</strong> An interest in exploring advanced probability concepts, such as sigma algebras and probability spaces, introduced in an accessible way.  </p>
<p><strong>Optional:</strong> Familiarity with calculus or logical reasoning can enhance understanding but isn't required.  </p>
<p>No prior knowledge of measure theory or advanced probability is needed&mdash;we'll build from intuitive ideas to formal concepts together!</p>
<h2 id="the-paradox">The Paradox</h2>
<p>Let's start with a question that seems pretty simple, but is deceptively paradoxical with the math taught in high school.   </p>
<p>According to <a href="https://en.wikipedia.org/wiki/Bertrand_paradox_(probability)">Wikipedia</a> it may be formulated as follows: </p>
<blockquote>
<p>"Consider an equilateral triangle that is inscribed in a circle. Suppose a chord of the circle is chosen at random. What is the probability that the chord is longer than a side of the triangle?"</p>
</blockquote>
<p>This formulation makes it seem like quite an abstract question,   </p>
<p>So we may present it alternatively as: </p>
<blockquote>
<p>Consider two concentric circles with <code>A</code> and <code>B</code> with radius <code>R</code> and <code>2R</code>. Given a chord of the circle <code>B</code> is picked at random, find the probability that this chord intersects, the inner circle <code>A</code>.</p>
</blockquote>
<p><br></p>
<p><img src="/static/images/mathforquant/math-for-quant-part-1-1.svg" style="height: 300px !important" class="invertible"></p>
<p><br></p>
<p>This reformulation may model practical scenarios, such as piercing a padded sphere to assess whether it impacts an inner core, though this is merely a contextual preference and does not alter the mathematical problem.</p>
<p>Now typically there are <strong>Three methods</strong> of tackling this problem. </p>
<h3 id="method-1-random-midpoint-method">Method 1: Random Midpoint Method</h3>
<p>This is the method that occurred to me, when I tried to solve this problem. </p>
<p>If one plays around with the problem long enough, drawing multiple sketches, we might notice this&mdash;</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-2.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>The <strong>midpoints</strong> of all chords that intersect the inner circle, <strong>must lie inside the inner circle</strong>.  </p>
<p>Since any chord $ AB $ of the outer circle that intersects the inner circle must have its perpendicular distance $ d \leq R $, its midpoint $ M $ (at distance $ d $ from $ O $) must lie within or on the inner circle.</p>
<p>Now one can definitely prove this rigorously (and it is a trivial geometric proof), but for our case, let's just say it is true, visually.</p>
<p>Now if we take only the mid points into consideration, and define a coordinate system with the origin at the centre of $ A , B $, we get a sample space that looks like this&mdash;</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-3.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>And if we draw our required event on it, we get&mdash;</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-4.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>So for some formal definitions&mdash; <br />
$$
\begin{aligned}
\Omega &amp;= \{(x,y) \in \mathbb{R}^2 \mid x^2 + y^2 \leq (2R)^2\} \newline
\mathbb{E} &amp;= \{(x,y) \in \mathbb{R}^2 \mid x^2 + y^2 \leq R^2\}
\end{aligned}
$$</p>
<p>Now, to calculate $ P(\mathbb{E}) $ we can simply do&mdash; </p>
<p>$$ 
P(\mathbb{E}) = \frac{\pi(R)^2}{\pi(2R)^2} = \frac{1}{4}
$$</p>
<p>So our $ P(\mathbb{E}) $ is &mdash;</p>
<p>$$
P(\mathbb{E}) = \boxed{\frac{1}{4}}
$$</p>
<h3 id="method-2-diameter-method">Method 2: Diameter Method</h3>
<p>We may also try a more methodical way&mdash;</p>
<p>We first pick a point on the circumference of $ B $, say $ P $,  then we draw a <strong>diameter</strong> passing through the center. Now we may vary this line, by an angle $ \theta $ and get all possible chords that pass through the point $ P $. Similarly we may do this for every point on the circumference of $ B $. </p>
<p>here is what it looks like&mdash;</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-5.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>here is the visualization of $ \Omega $ and $ \mathbb{E} $ (NOTE: here X is the distance along the circumference of B)</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-6.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>Why $ \frac{\pi}{6} $?<br />
We can clearly see that a right angled triangle is formed when the chord just touches the inner circle (it is tangent). The hypotenuse is $ 2R $ and the base is $ R $. So we may do $ \cos(\theta) = \frac{R}{2R} $ and then inverse it to get $ \theta = \frac{\pi}{6}$.  </p>
<p>Formally&mdash;
$$
\begin{aligned}
\Omega &amp;= \{(X, \theta) \in \mathbb{R}^2 \mid X \in (-2\pi R, 2\pi R), \theta \in (0, \pi/2)\} \newline
\mathbb{E} &amp;= \{(X, \theta) \in \mathbb{R}^2 \mid X \in (-2\pi R, 2\pi R), \theta \in (0, \pi/6)\}
\end{aligned}
$$</p>
<p>Or&mdash;
$$
\begin{aligned}
\Omega &amp;= (-2\pi R, 2\pi R) \times (0, \pi/2) \newline
\mathbb{E} &amp;= (-2\pi R, 2\pi R) \times (0, \pi/6)
\end{aligned}
$$</p>
<p>We can now calculate the probability, like&mdash;
$$
\begin{aligned}
P(\mathbb{E}) &amp;= \frac{4\pi R \cdot (\pi/6)}{4\pi R \cdot (\pi/2)} = \frac{1}{3} \newline
P(\mathbb{E}) &amp;= \boxed{\frac{1}{3}}
\end{aligned}
$$</p>
<h3 id="method-3-radius-method">Method 3: Radius Method</h3>
<p>Take a chord passing through the center at an elevation $ \theta $ from the horizontal. Now we may consider all parallel chords to the diameter, at that elevation, their perpedicular distance from the center being $ x $. We get&mdash;</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-7.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>and the visualization&mdash;
<br>
<img src="/static/images/mathforquant/math-for-quant-part-1-8.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>The rest of it is elementary, and $ P(\mathbb{E}) $ comes out to&mdash;
$$
P(\mathbb{E}) = \boxed{\frac{1}{2}}
$$</p>
<h3 id="the-contradiction">The Contradiction</h3>
<p>So, we have <strong>Three answers</strong>. This is clearly a contradiction. It means somewhere along the way, our approach with <strong>highschool math</strong> was <strong>flawed</strong>. This problem is called the <strong>Bertrand Paradox</strong>. To correct this we would need an even deeper understanding of probability theory, beyond the typical highschool math.  <br />
In reality it is the question that may be misunderstood, the right interpretation is crucial to finding the solution to this paradox. We must realise that <strong>the choice of our random chord can impact the probability measure</strong>. There are <strong>different kinds of random</strong>.  </p>
<p>Unfortuanately, we need the proper "advanced probability theory" to properly discuss the resolution, which is what we'll be exploring in this article. I might write a follow-up post for the resolution as this post is already going to get pretty long. If you really want to know the in-depth reason you can check out the <a href="https://en.wikipedia.org/wiki/Bertrand_paradox_(probability)">Wikipedia Page</a> <br />
(HINT: the answer, for uniform distributions, is $ \frac{1}{2} $)</p>
<h1 id="terms-and-definitions">Terms and Definitions</h1>
<p>Understanding the terms and notation is the most crucial part to understanding probability. </p>
<p>Let's try to intuitively find out what are the terms required to make a reasonable comment on the probability of an event.  </p>
<h2 id="what-is-probability">What is Probability?</h2>
<p>One could call it the <strong>likelihood or chance</strong>, of a certain event happening. (not to be confused with plausibility)    </p>
<p>What information would we need to comment on such a matter? Usually we would need to know what it is, whose outcome we are trying to determine&mdash; Is it a coin flip? a horse race? a cricket match?&mdash; This is is called the <strong>Random Experiment (RE)</strong>.    </p>
<p>Say we want to determine the probability that a grain of sand ends up exactly on top of a treasure chest, on a beach, after being washed away. Obviously we cannot make any reasonably accurate prediction of such an event. Why is that so? It is because we <strong>do not know</strong> the set of <strong>all possible outcomes</strong> of our random experiment (RE). So to make an accurate comment on the probability of an event, we must also know the set of all possible outcomes of our RE. This set is called the <strong>Sample Space</strong> ($ \Omega $)</p>
<h2 id="sample-space">Sample Space</h2>
<p>Now the sample space space could be something like&mdash;</p>
<p>$$
\Omega = \{Heads, Tails\}
$$</p>
<p>for a coin flip to something like&mdash;</p>
<p>$$
\Omega = \{1,2,3,4,5,6 \}
$$</p>
<p>for a dice roll.    </p>
<p>It may also be something complex like&mdash;</p>
<p>$$
\Omega = \{(x,y) \in \mathbb{R}^2 \mid 0 \leq x \leq 1, 0 \leq y \leq 1\}
$$</p>
<p>i.e any random point on a unit square. Even more complex &mdash;</p>
<p>$$
\Omega = \{(x,y) \in \mathbb{R}^2 \mid 0.25 \leq x^2 + y^2 \leq 1\}
$$</p>
<p>i.e any random point on a unit disc, that is atleast 0.5 units away from the center. It could also be visualized as a ring. </p>
<p>Finally, can you guess what this represents &mdash;</p>
<p>$$
\Omega = \{(x,y,z) \in \mathbb{R}^3 \mid x^2 + y^2 + z^2 = 1, z \geq 0, \sqrt{x^2 + y^2 + (z-1)^2} \leq 1\}
$$</p>
<p>Kind of scary isn't it?
( HINT: it's a sphere, with a special condition.)   </p>
<p>This should give an idea as to <em>why</em> we need a formal notation for something so intuitive. As we will see later, the theory builds on this and becomes powerful enough to over-take the intuitive approach.</p>
<h2 id="sigma-algebra">Sigma Algebra</h2>
<p>Typically this is part of <strong>measure theory</strong>, so we'll only cover what we absolutely need to know.</p>
<p>Again <a href="https://en.wikipedia.org/wiki/%CE%A3-algebra">Wikipedia</a> says:</p>
<blockquote>
<p>In formal terms, a $\sigma$-algebra on a set $ X $ is a nonempty collection $ \Sigma $ of subsets of $ X $ closed under complement, countable unions, and countable intersections. The ordered pair ( $ X $, $ \Sigma $) is called a measurable space.</p>
</blockquote>
<p>Too Complicated, Didn't Understand (TC;DU)</p>
<p>Let's start with the simple problem of trying to have complete knowledge of all events that can possibly occur. How do we find this? simple its just all possible subsets of our sample space $ \Omega $&mdash; But can we assign probabilities to <strong>all</strong> of these events? </p>
<p>In some cases, no, we can't.</p>
<p>To put it simply think of something like this&mdash;</p>
<p>we have $ \Omega = [0,1] $, now if we say that we want an event ($ E $) where the <strong>length of the interval is 0.5</strong>.  <br />
How many such subsets do you think there are? infinite. It could be $ [0.2, 0.7] $, $ [0.3, 0.8] $, and so on.  </p>
<p>Now whats $ P(E)_{[0.2 , 0.7]} $? 
Common sense would say its <strong>0.5</strong> since it covers exactly half of the sample space. But this is where intuition breaks down, for example let's try the <strong>vitali set</strong>&mdash;</p>
<p>(NOTE: In hindsight, the exploration coming up wasn't necessary, you may skip it if you like.)</p>
<p><br></p>
<hr>
<p><br></p>
<h3 id="vitali-set-deep-dive">Vitali Set (Deep Dive)</h3>
<p>Suppose we organize the numbers $(x,y)$ from $[0,1]$ into groups based on a set condition, i.e, $ x - y \in \mathbb{Q} $, the difference must be rational. So we could have $0.7 - 0.2 = 0.5$, in the same group, and $0.8 - 0.3 = 0.5$ in the same group.<br />
This splits $[0,1]$ into a <strong>uncountably many groups</strong>, for instance, one group might include $0.2, 0.7, 0.2 + 1/3, 0.7 + 1/3,$ and so on, as long as the differences are rational. </p>
<p>Suppose, we pick <em>exactly one</em> number from each group to create a set $ V $, called the Vitali set. The rule is that the numbers we pick must not differ by a rational number, because if they did (like 0.2 and 0.7), they'd be in the same group, and we're only allowed one number per group. So something like $ 0.2 $, and $ 1/\sqrt{2} $ etc. This is tricky because there are uncountably many groups, and we rely on the axiom of choice to ensure we can pick one number from each.    </p>
<p>So, we've got our Vitali set $ V $, a collection of numbers, one from each group, where no two numbers differ by a rational. Why does this cause trouble? Let's try to assign it a probability $ P(V) = a $. To check if this works, we shift $ V $ by all rational numbers in $ [0, 1] $, like 0, 0.1, 0.2, 0.3, and so on. For each rational $ q $, the set $ V + q $ (where we add $ q $ to each number in $ V $, wrapping around to stay in $ [0, 1] $) is a shifted copy of $ V $. These shifted sets are <strong>disjoint</strong>&mdash;they don't overlap. Why? If some number appeared in both $ V + q_1 $ and $ V + q_2 $, say $ x + q_1 = y + q_2 $, then $ x - y = q_2 - q_1 $, a rational number, which would mean $ x $ and $ y $ are in the same group, contradicting the fact that $ V $ has only one number per group.</p>
<p>The union of all these shifted sets $ V + q $, over all rational $ q \in [0, 1] $, covers all of $ [0, 1] $. Why? Every number in $ [0, 1] $ belongs to some group, and by shifting $ V $'s representative from each group by all possible rational numbers, we hit every number in every group. Since $ P([0, 1]) = 1 $, the probability of this union should be 1. By our probability rule of countable additivity, the probability of the union is the sum of the probabilities of these disjoint sets:</p>
<p>$$ P\left( \bigcup_{q} (V + q) \right) = \sum_{q} P(V + q) $$</p>
<p>Since shifting by a rational doesn't change the "size" of the set (in a uniform measure), each $ V + q $ should have the same probability as $ V $, so $ P(V + q) = a $. There are countably many rational numbers in $ [0, 1] $, so the sum is:</p>
<p>$$ \sum_{q} P(V + q) = \sum_{q} a $$</p>
<p>Now, we're stuck:</p>
<ul>
<li>If $ a &gt; 0 $, the sum $ a + a + \dots $ over countably many terms is infinite, which can't equal 1. That's a contradiction.</li>
<li>If $ a = 0 $, the sum is 0, which also can't equal 1. Another contradiction.</li>
</ul>
<p>This shows we <em>can't</em> assign a probability to $ V $ without breaking our probability rules. The Vitali set is <strong>non-measurable</strong>-it's too strange to fit into our probability framework. This is why we can't assign probabilities to <em>every</em> subset of $ \Omega = [0, 1] $. We need a <strong>sigma algebra</strong> to limit ourselves to subsets that play nice.
<br></p>
<hr>
<p><br>
A sigma algebra $ \mathcal{F} $ is our curated list of "good" subsets-events we can assign probabilities to consistently. It follows three rules:</p>
<ol>
<li><strong>The whole space is included</strong>: $ \Omega \in \mathcal{F} $. We need to say "something happens" with probability 1.</li>
<li><strong>Closed under complements</strong>: If $ A \in \mathcal{F} $, then $ \Omega \setminus A \in \mathcal{F} $ ($\Omega$ - A$). If "the number is in $ [0.2, 0.7] $" is an event, "the number is <em>not</em> in $ [0.2, 0.7] $" (i.e., $ [0, 0.2) \cup (0.7, 1] $) is too.</li>
<li><strong>Closed under countable unions</strong>: If $ A_1, A_2, \dots \in \mathcal{F} $, then $ \bigcup_{i=1}^\infty A_i \in \mathcal{F} $. This lets us combine events like $ [0.1, 0.2] \cup [0.3, 0.4] $.</li>
</ol>
<p>For $ \Omega = [0, 1] $, a sigma algebra includes nice subsets like intervals, their unions, and complements, but leaves out non-measurable sets like the Vitali set.</p>
<p>But for our intents and purposes, one may think of this as a <strong>power set</strong>, for simplicity.</p>
<h2 id="the-borel-sigma-algebra">The Borel Sigma Algebra</h2>
<p><strong>TL:DR</strong> It's just a sigma algebra but open intervals.</p>
<p>So, what's a <strong>Borel sigma algebra</strong>? It's the standard sigma algebra for continuous spaces like $ [0, 1] $, $ \mathbb{R} $, or $ \mathbb{R}^2 $, used in problems like the Bertrand paradox. Think of it as the ultimate collection of measurable subsets that makes probability calculations safe and practical.</p>
<p>The Borel sigma algebra, denoted $ \mathcal{B} $, starts with all <strong>open sets</strong> in your space. In $ \Omega = [0, 1] $, open sets are intervals like $ (0.2, 0.5) $ (not including endpoints). In $ \mathbb{R}^2 $, they're open disks, rectangles, or shapes with smooth boundaries. The Borel sigma algebra is the smallest sigma algebra that contains all these open sets, built by:</p>
<ol>
<li>Including all open sets.</li>
<li>Adding their complements (closed sets, like $ [0.2, 0.5] $).</li>
<li>Adding all countable unions and intersections of these sets.</li>
</ol>
<p>This creates a huge collection of measurable sets (called <strong>Borel Sets</strong>), including:</p>
<ul>
<li>Intervals: $ [0.2, 0.5] $, $ (0, 1) $, or single points like $ {0.42} $.</li>
<li>Unions of intervals: $ [0.1, 0.3] \cup [0.5, 0.7] $.</li>
</ul>
<h2 id="probability-measures-and-probability-spaces">Probability Measures and Probability Spaces</h2>
<h3 id="probability-measures">Probability Measures</h3>
<p>So we laid down most of the formal stuff, now to actually define our <strong>Probability measure</strong>, we may take a function&mdash;</p>
<p>$$
P: \mathcal{F} \to [0,1]
$$</p>
<p>From domain = $\mathcal{F}$, to codomain/range = $[0,1]$. Note this assigns probabilities to <strong>events in</strong> $ \mathcal{F} $(similar to what is covered in highschool), and <strong>is different from PMFs and PDFs</strong> covered later on. 
It must satisfy:    </p>
<ol>
<li>$P(\Omega) = 1$  </li>
<li>$P(\phi) = 0$</li>
<li><strong>Countable additivity</strong>: For a countable collection of <strong>disjoint</strong> sets, $A_1, A_2, \dots \in \mathcal{F}$, $$  P(\bigcup_{i=1}^\infty A_i) = \Sigma_{i=1}^\infty P(A_i) $$</li>
</ol>
<p>If all of this seems obvious, good. But in most cases it would be advisable to think of continuous probability as different from discrete probability.</p>
<h3 id="probability-spaces">Probability Spaces</h3>
<p>So from the terms we defined above, now we can finally define our "world", a probability space. Let's simply package all the relevant stuff into a triple of $(\Omega, \mathcal{F}, P)$.    </p>
<p>For example, for a (single) fair coin flip&mdash;</p>
<p>$$
\begin{aligned}
\Omega &amp;= \{H, T\} \newline
\mathcal{F} &amp;= \{\phi, \{H\}, \{T\}, \{H, T\}\} \newline
P&amp;(\{H\}) = 0.5 , P({\{T\}}) = 0.5
\end{aligned}
$$</p>
<h2 id="random-variables">Random Variables</h2>
<p>So we have a sample space, but doing calculations on it could be quite difficult, especially because we don't really have any set <strong>ordering</strong>. We typically want something in terms of <strong>numbers</strong> to do anything practical with it. So we introduce <strong>Random Variables (RVs)</strong>, that solve this problem.</p>
<p>Despite the name, a Random Variable is actually a <strong>function</strong>&mdash; <br />
$$
X: \Omega \to \mathbb{R}
$$</p>
<p>It assigns a <strong>Real Number</strong> ($\mathbb{R}$) to every outcome in the sample space ($\Omega$).  <br />
Now obviously we can have <strong>two types</strong> of random variables.    </p>
<h3 id="discrete-random-variables">Discrete Random Variables</h3>
<p>It usually takes countable values, for example let's take a dice rolled 3 times. Then&mdash;</p>
<p>$$
\Omega = \{(i,j,k): i,j,k \in {1,2,3,4,5,6}\}
$$</p>
<p>and suppose our random variable ($X$) is the number of 6s, rolled in the 3 tries&mdash;</p>
<p>$$
X: \Omega \to \{0,1,2,3\}
$$</p>
<p>and also&mdash;</p>
<p>$$
X(\omega \in \Omega) = \text{ number of sixes}
$$</p>
<p>More formally&mdash;</p>
<p>$$
X((i,j,k)) = \Sigma_{m\in\{i,j,k\}} 1_{\{6\}}(m)
$$</p>
<p>where $1_{\{6\}}(m)$ is an indicator function, defined as:    </p>
<p>$$
1_{\{6\}}(m) = 
\begin{cases} 
1 &amp; \text{if } m = 6, \newline
0 &amp; \text{if } m \neq 6
\end{cases}
$$</p>
<p>But anyways, there are many ways to define a RV.    </p>
<p>Then where is the $\sigma$-algebra ($\mathcal{F}$)? This is the question that confused me for a long time.<br />
Think of it something like this&mdash;<br />
In the dice roll example above, if we want the probability of the event that the RV takes the value $X=1$ (i.e $P(X=1)$)  <br />
We would want the probability of the event defined by the set&mdash;
$$
\{\omega \in \Omega: X(\omega) = 1\}
$$  </p>
<p>This looks some thing like $\{(2,6,4),(4,6,5) \dots\}$. Now this set must belong to $\mathcal{F}$, i.e, <br />
$$
\{\omega \in \Omega: X(\omega) = 1\} \in \mathcal{F}
$$</p>
<p>This ensures our event is <strong>measurable</strong> (refer to the Vitali set above for the counter example). But when applied to most real-world scenarios, one doesn't have to actively check for this statement to be true.  </p>
<p>Also note, here the $ X $ maps from $ \Omega $ to a <strong>Borel Set</strong>, in this case $\{0,1,2,3\}$</p>
<h3 id="continous-random-variables">Continous Random Variables</h3>
<p>More or less similar to the discrete type, except&mdash;</p>
<p>$$
X: \Omega \to \mathbb{R}^n \text{ Where }  n \in \mathbb{Z}
$$</p>
<p>The rest can be inferred. Maybe try to guess what the RV was in the Bertrand Paradox stated earlier.</p>
<h1 id="probability-distributions">Probability distributions</h1>
<h2 id="the-pmf-and-the-pdf">The PMF and the PDF</h2>
<p>So, we've defined random variables, which turn outcomes in $\Omega$ into numbers, and we've set up our probability space ($\Omega, \mathcal{F}, P$) to assign probabilities to events. But how do we figure out the likelihood of a random variable taking specific values, like "exactly one 6 in three dice rolls" or "the chord's midpoint in the inner circle"? 
This is where <strong>probability distributions</strong> come in&mdash;they describe how probabilities are spread across the values of a random variable.    </p>
<p>The <strong>probability measure</strong> $ P $, assigns probabilities to <strong>events</strong> in $ \mathcal{F} $, but as we've seen before RVs aren't events. So just to compute the <strong>probability of the subset of events the RV takes</strong> directly (like $P(X=1)$), we introduce tools like the <strong>PMF and PDF</strong>.</p>
<h3 id="probability-mass-function-pmf">Probability Mass Function (PMF)</h3>
<p>For a <strong>discrete random variable</strong>, which takes countable values (like the number of 6s in three dice rolls), the probability mass function (PMF) gives the probability of each possible value: </p>
<p>$$
P(X = x)
$$  </p>
<p>Let's revisit our dice example. The sample space is&mdash;</p>
<p>$$
\Omega = \{(i,j,k): i,j,k \in {1,2,3,4,5,6}\}
$$</p>
<p>And $X((i,j,k)) = \Sigma_{m\in\{i,j,k\}} 1_{\{6\}}(m)$, where $1_{\{6\}}(m)$ is&mdash;    </p>
<p>$$
1_{\{6\}}(m) = 
\begin{cases} 
1 &amp; \text{if } m = 6, \newline
0 &amp; \text{if } m \neq 6
\end{cases}
$$</p>
<p>Here the PMF tells us probabilities like&mdash; $ P(X=1) $, i.e only one 6 is rolled, in three tries. Now for simple stuff like this dice roll, even highschool math will suffice, but just to put it out there, $X$ follows a <strong>binomial distribution</strong> (Will be covered ahead). So the PMF looks something like&mdash; </p>
<p>$$
P(X=k) = \binom{3}{k}\cdot\left(\frac{1}{6}\right)^k\cdot\left(\frac{5}{6}\right)^{3-k}  \text{ where }   k \in \{0,1,2,3\} 
$$</p>
<p>Here $k = 1$ gives&mdash;</p>
<p>$$
P(X=1) = \binom{3}{1}\cdot\left(\frac{1}{6}\right)^1\cdot\left(\frac{5}{6}\right)^{2} = \frac{75}{216} \approx 0.347
$$  </p>
<h3 id="probability-density-function-pdf">Probability Density Function (PDF)</h3>
<p>For a <strong>continuous random variable</strong>, which takes values in a continuum (like the coordinates of a chord's midpoint in the Bertrand paradox), we can't assign probabilities to specific values because $ P(X = x) = 0 $. So what, do we do? Is it just not possible? Of course not! We take an interval of values, like $ P(a \leq X \leq b) $. This should give us the probability that $ X $ lies in the interval $ [a , b] $, which usually won't be zero (To serve as an abstraction, one can think of this as taking a <strong>small section</strong> of a line, rather than a <strong>point</strong> on a line. The point will have no length on the line, which makes defining probabilities for it quite difficult, but the small section has some finite length, however small it may be).   </p>
<p>How do we do this though? Simple. We define another function, called the <strong>probability density function (PDF)</strong> ( $ f(x) $ ). We want this function to be some kind of a "density map" of sorts.    </p>
<p>So obviously to find the probability of a specific part we'd do&mdash;  </p>
<p>$$
P(a \leq X \leq b) = \frac{\int_{a}^{b} f(x)dx}{\int_{-\infty}^{\infty} f(x)dx}
$$</p>
<p>But that does not look pretty, especially since $ f(x) $ is not <strong>normalized</strong>. So we conviniently define $ f(x) $ in such a way that&mdash;    </p>
<p>$$
\int_{-\infty}^{\infty} f(x)dx = 1
$$</p>
<p>So now we get&mdash;</p>
<p>$$
P(a \leq X \leq b) = \int_{a}^{b} f(x)dx
$$</p>
<p>Also it must be <strong>non-negative</strong> for all $x$. i.e $f(x)\geq 0 \forall x$ (try to justify this claim, it's pretty intuitive, but the proof is still quite long and excluded for brevity).    </p>
<p>Let's do some examples because abstract definitions like this make no sense.    </p>
<hr />
<h4 id="example-1-uniform-distribution-on-an-interval-1d">Example 1: Uniform Distribution on an Interval (1D)</h4>
<p><strong>Scenario</strong>: Suppose you're waiting for a bus that arrives randomly between 0 and 10 minutes from now. Let $X$ be the waiting time in minutes, uniformly distributed over $[0, 10]$.</p>
<p><strong>PDF</strong>: Since $X$ is equally likely to take any value in $[0, 10]$, the PDF is constant over that interval. The total area under the PDF must be 1:</p>
<p>$$
f(x) = \frac{1}{10}, \quad 0 \leq x \leq 10
$$</p>
<p>(Outside $[0, 10]$, $f(x) = 0$.) Check: $\int_0^{10} \frac{1}{10} \, dx = \frac{10}{10} = 1$.</p>
<p><strong>Probability</strong>: What's the probability you wait between 2 and 5 minutes ($P(2 \leq X \leq 5)$)?</p>
<p>$$
P(2 \leq X \leq 5) = \int_2^5 \frac{1}{10} \, dx = \frac{5 - 2}{10} = \frac{3}{10} = 0.3
$$</p>
<p><strong>Intuition</strong>: The PDF is a flat line at height $\frac{1}{10}$. The interval $[2, 5]$ has length 3, so the area is $3 \cdot \frac{1}{10} = 0.3$.</p>
<p>It looks something like&mdash;</p>
<p><br></p>
<p><img src="/static/images/mathforquant/math-for-quant-part-1-9.svg" style="height: 300px !important" class="invertible"></p>
<p><br>
<br></p>
<hr />
<p><br>
For the sake of brevity, we'll go straight to the named distributions from here, but maybe try to think up a few more examples, before we do. Maybe try to get an exponential distribution (i.e in terms of $e^k \text{ or } a^k$)? (Remember you must normalize them in some way to get $f(x)$)</p>
<h2 id="the-binomial-distribution">The Binomial distribution</h2>
<p>We've already seen the binomial distribution sneak into our dice example, where we calculated the probability of rolling exactly one 6 in three tries. But what exactly is this distribution, and why is it so important? Let's break it down.  </p>
<p>Let's start with the main question:</p>
<blockquote>
<p>Given a fixed number of trials (<code>n</code>), flipping a fair coin, calculate the probability of getting exactly 1 heads in these <code>n</code> trials.</p>
</blockquote>
<p>Simple high school math right?<br />
But let's visualize it&mdash;</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-10.svg" style="height: 300px !important" class="invertible">
<br></p>
<p>Notice that if we select a path all the way down to the bottom, the probabilities of each branch get <strong>multiplied</strong>. In our case, as it's a fair coin, $ p = (1-p) = 0.5 $.
here we <strong>only need 1 heads</strong>, so we must continue on the rightmost branch, and select the place where we get our heads. We can select the "branch" where we make the split with&mdash;</p>
<p>$$
\binom{n}{1} = n
$$</p>
<p>And our final probability will look like&mdash;</p>
<p>$$
\binom{n}{1}\cdot(0.5)^1\cdot(0.5)^{(n-1)}
$$</p>
<p>So to generalize this:<br />
$$
\begin{aligned}
p:&amp; \text{ Probability of success.} \newline
n:&amp; \text{ Number of trials} \newline
k:&amp; \text{ Required number of successes}
\end{aligned}
$$</p>
<p>So we get a RV, $X$, and&mdash;
$$
P(X=k) = \binom{n}{k}\cdot(p)^k\cdot(1-p)^{(n-k)}
$$</p>
<p>Here is the graph ($n=10, p=0.5$)&mdash;
<br></p>
<iframe src="https://www.desmos.com/calculator/uwq5nondpt?embed" width="100%"  height="500" style="border: 1px solid #ccc;border-radius:5px" frameborder=0></iframe>
<p><br></p>
<p>The green lines are the floored values of $X$, i.e Integers, where as the red curve is the extension of this binomial distribution. Can you point out what's wrong with it? yeah. The red graph may cause confusion, as the binomial distribution is <strong>discrete</strong> in nature, so it shouldn't have a continuous graph.    </p>
<p>[TODO]</p></div>
</article>

        </main>
    </div>
</body>

</html>
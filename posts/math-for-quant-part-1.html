<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="google-site-verification" content="QMvy4LUd9kqEbkLgKUPNyuYlMu0E12nfHPE_xX4zg9I" />
    <title>Beyond High School Probability: Unlocking Binomial, Gaussian, and More</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@700&family=Open+Sans&family=Lora&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="/static/css/style.css">
    <link rel="stylesheet" href="/static/css/pygments.css">
    <!-- MathJax Configuration -->
    <script>
        MathJax = {
            tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] },
            svg: { fontCache: 'global' }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="/static/js/main.js" defer></script>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0Z497HS9D6"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-0Z497HS9D6');
    </script>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Sans+JP&display=swap" rel="stylesheet">
</head>

<body>
    <div class="container">
        <nav class="navbar">
            <a href="/" class="logo">MONOQ</a>
            <div class="hamburger">â˜°</div>
            <div class="nav-content">
                <ul class="main-nav">
                    <li><a href="/">Home</a></li>
                    <li><a href="/posts/">Posts</a></li>
                    <li><a href="/highlights/">Highlights</a></li>
                    <li><a href="/about/">About Me</a></li>
                </ul>
                <div class="social-links">
                    <a href="https://github.com/kyuqee" target="_blank">
                        <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png"
                            alt="GitHub" class="social-icon invertible">
                    </a>
                    <a href="mailto:kyuqeg@gmail.com">
                        <img src="https://cdn-icons-png.flaticon.com/512/561/561127.png" alt="Email"
                            class="social-icon invertible">
                    </a>
                </div>
                <button id="theme-toggle">Toggle Theme</button>
            </div>
        </nav>
        <main>
            
<article>
    <h1>Beyond High School Probability: Unlocking Binomial, Gaussian, and More</h1>
    <p>2025-06-11</p>
    <br>

    
    <div class="toc-wrapper">
        <h2>Table of Contents</h2>
        <div class="toc">
            <div class="toc">
<ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#resources">Resources</a></li>
<li><a href="#prerequisites-for-reading">Prerequisites for Reading</a></li>
<li><a href="#the-paradox">The Paradox</a><ul>
<li><a href="#method-1-random-midpoint-method">Method 1: Random Midpoint Method</a></li>
<li><a href="#method-2-diameter-method">Method 2: Diameter Method</a></li>
<li><a href="#method-3-radius-method">Method 3: Radius Method</a></li>
<li><a href="#the-contradiction">The Contradiction</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#terms-and-definitions">Terms and Definitions</a><ul>
<li><a href="#what-is-probability">What is Probability?</a></li>
<li><a href="#sample-space">Sample Space</a></li>
<li><a href="#sigma-algebra">Sigma Algebra</a><ul>
<li><a href="#vitali-set-deep-dive">Vitali Set (Deep Dive)</a></li>
</ul>
</li>
<li><a href="#the-borel-sigma-algebra">The Borel Sigma Algebra</a></li>
<li><a href="#probability-measures-and-probability-spaces">Probability Measures and Probability Spaces</a><ul>
<li><a href="#probability-measures">Probability Measures</a></li>
<li><a href="#probability-spaces">Probability Spaces</a></li>
</ul>
</li>
<li><a href="#random-variables">Random Variables</a><ul>
<li><a href="#discrete-random-variables">Discrete Random Variables</a></li>
<li><a href="#continuous-random-variables">Continuous Random Variables</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#probability-distributions">Probability distributions</a><ul>
<li><a href="#the-pmf-and-the-pdf">The PMF and the PDF</a><ul>
<li><a href="#probability-mass-function-pmf">Probability Mass Function (PMF)</a></li>
<li><a href="#probability-density-function-pdf">Probability Density Function (PDF)</a><ul>
<li><a href="#example-1-uniform-distribution-on-an-interval-1d">Example 1: Uniform Distribution on an Interval (1D)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#the-binomial-distribution">The Binomial distribution</a><ul>
<li><a href="#mean-expected-value">Mean (Expected Value)</a></li>
<li><a href="#variance">Variance</a></li>
</ul>
</li>
<li><a href="#the-poisson-distribution">The Poisson distribution</a><ul>
<li><a href="#deriving-the-poisson-from-the-binomial">Deriving the Poisson from the Binomial</a></li>
<li><a href="#mean-expected-value-variance">Mean (Expected Value) &amp; Variance</a></li>
</ul>
</li>
<li><a href="#the-gaussian-normal-distribution">The Gaussian (Normal) distribution</a><ul>
<li><a href="#deriving-the-gaussian-from-the-binomial">Deriving the Gaussian from the Binomial</a></li>
<li><a href="#mean-and-variance">Mean and Variance</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#conclusion">Conclusion</a><ul>
<li><a href="#recap">Recap</a></li>
<li><a href="#personal-remarks">Personal Remarks</a></li>
</ul>
</li>
</ul>
</div>

        </div>
    </div>
    
    <br>
    <div><p>(NOTE: This turned out to be quite a long post, it might take around an hour to read fully)</p>
<h1 id="introduction">Introduction</h1>
<p>High school curricula typically introduce basic probability concepts; however, probability theory applied in financial mathematics and statistics requires significantly advanced techniques. To enhance my understanding and proficiency, I will undertake an in-depth exploration of probability theory, encompassing distributions such as Binomial, Gaussian, and Poisson, as well as concepts including random variables, sigma algebras, and probability spaces.</p>
<p>This is by no means a proper text, or learning resource. This is simply what <strong>I would've wanted to read</strong>, when I was getting started with these topics. Here are the resources I used to learn these topics (still learning, but more or less understood the basics).</p>
<h2 id="resources">Resources</h2>
<ol>
<li><strong>MIT OCW 18.S096</strong>: <a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP63ctJIEC1UnZ0btsphnnoHR">youtube link</a>, <a href="https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-with-applications-in-finance-fall-2013/">website link</a>. It's pretty good, but required a lot of previous undergrad knowledge so the learning curve was quite steep. It did however introduce me to the right topics to learn, and I learnt them all individually from different resources.</li>
<li><strong>Probability and Stochastics for finance, NPTEL</strong>: <a href="https://www.youtube.com/@probabilityandstochasticsf5278">youtube link</a>, Extremely good, especially since it starts right from the highschool level. This is also where I got the inspiration for the Bertrand Paradox, presented ahead.</li>
<li><strong>Quantitative Finance 101, FEBS IIT Bhubaneshwar</strong>: <a href="https://www.youtube.com/playlist?list=PLX5wiDP8rz62dRofitFcP10iqLTjLKfmq">youtube link</a>. This made me understand sigma algebras, atleast at a level where I can apply them reasonably. It doesn't really assume zero knowledge, but is pretty in depth.    </li>
<li><strong>Shreve Stochastic Calculus for Finance II: Continuous-Time Models</strong>: <a href="https://g.co/kgs/n3SQKnz">link</a>. This book was recommended multiple times, no matter where I looked on the internet. It is in-fact the go-to book for this topic. </li>
</ol>
<p>Apart from these I've used several other miscellaneous sources, which I won't state here for the sake of brevity.</p>
<h2 id="prerequisites-for-reading">Prerequisites for Reading</h2>
<p><strong>High School Math:</strong> A solid foundation in algebra, coordinate geometry, trigonometry, and basic probability (e.g., AP Calculus/Statistics, JEE Advanced, IB Math HL, or A-Level Mathematics). </p>
<p><strong>Set Notation:</strong> Comfort with basic set notation (e.g., describing points in a plane or intervals).    </p>
<p><strong>Geometric Intuition:</strong> Ability to visualize geometric shapes like circles and chords. </p>
<p><strong>Curiosity:</strong> An interest in exploring advanced probability concepts, such as sigma algebras and probability spaces, introduced in an accessible way.  </p>
<p>No prior knowledge of measure theory or advanced probability is needed&mdash;we'll build from intuitive ideas to formal concepts together!</p>
<h2 id="the-paradox">The Paradox</h2>
<p>Let's start with a question that seems pretty simple, but is deceptively paradoxical with the math taught in high school.   </p>
<p>According to <a href="https://en.wikipedia.org/wiki/Bertrand_paradox_(probability)">Wikipedia</a> it may be formulated as follows: </p>
<blockquote>
<p>"Consider an equilateral triangle that is inscribed in a circle. Suppose a chord of the circle is chosen at random. What is the probability that the chord is longer than a side of the triangle?"</p>
</blockquote>
<p>This formulation makes it seem like quite an abstract question,   </p>
<p>So we may present it alternatively as: </p>
<blockquote>
<p>Consider two concentric circles with <code>A</code> and <code>B</code> with radius <code>R</code> and <code>2R</code>. Given a chord of the circle <code>B</code> is picked at random, find the probability that this chord intersects, the inner circle <code>A</code>.</p>
</blockquote>
<p><br></p>
<p><img src="/static/images/mathforquant/math-for-quant-part-1-1.svg" style="height: 300px !important" class="invertible"></p>
<p><br></p>
<p>This reformulation may model practical scenarios, such as piercing a padded sphere to assess whether it impacts an inner core, though this is merely a contextual preference and does not alter the mathematical problem.</p>
<p>Now typically there are <strong>Three methods</strong> of tackling this problem. </p>
<h3 id="method-1-random-midpoint-method">Method 1: Random Midpoint Method</h3>
<p>This is the method that occurred to me, when I tried to solve this problem. </p>
<p>If one plays around with the problem long enough, drawing multiple sketches, we might notice this&mdash;</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-2.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>The <strong>midpoints</strong> of all chords that intersect the inner circle, <strong>must lie inside the inner circle</strong>.  </p>
<p>Since any chord $ AB $ of the outer circle that intersects the inner circle must have its perpendicular distance $ d \leq R $, its midpoint $ M $ (at distance $ d $ from $ O $) must lie within or on the inner circle.</p>
<p>Now one can definitely prove this rigorously (and it is a trivial geometric proof), but for our case, let's just say it is true, visually.</p>
<p>Now if we take only the mid points into consideration, and define a coordinate system with the origin at the centre of $ A , B $, we get a sample space that looks like this&mdash;</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-3.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>And if we draw our required event on it, we get&mdash;</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-4.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>So for some formal definitions&mdash; <br />
$$
\begin{aligned}
\Omega &amp;= \{(x,y) \in \mathbb{R}^2 \mid x^2 + y^2 \leq (2R)^2\} \newline
\mathbb{E} &amp;= \{(x,y) \in \mathbb{R}^2 \mid x^2 + y^2 \leq R^2\}
\end{aligned}
$$</p>
<p>Now, to calculate $ P(\mathbb{E}) $ we can simply do&mdash; </p>
<p>$$ 
P(\mathbb{E}) = \frac{\pi(R)^2}{\pi(2R)^2} = \frac{1}{4}
$$</p>
<p>So our $ P(\mathbb{E}) $ is &mdash;</p>
<p>$$
P(\mathbb{E}) = \boxed{\frac{1}{4}}
$$</p>
<h3 id="method-2-diameter-method">Method 2: Diameter Method</h3>
<p>We may also try a more methodical way&mdash;</p>
<p>We first pick a point on the circumference of $ B $, say $ P $,  then we draw a <strong>diameter</strong> passing through the center. Now we may vary this line, by an angle $ \theta $ and get all possible chords that pass through the point $ P $. Similarly we may do this for every point on the circumference of $ B $. </p>
<p>here is what it looks like&mdash;</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-5.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>here is the visualization of $ \Omega $ and $ \mathbb{E} $ (NOTE: here X is the distance along the circumference of B)</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-6.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>Why $ \frac{\pi}{6} $?<br />
We can clearly see that a right angled triangle is formed when the chord just touches the inner circle (it is tangent). The hypotenuse is $ 2R $ and the base is $ R $. So we may do $ \cos(\theta) = \frac{R}{2R} $ and then inverse it to get $ \theta = \frac{\pi}{6}$.  </p>
<p>Formally&mdash;
$$
\begin{aligned}
\Omega &amp;= \{(X, \theta) \in \mathbb{R}^2 \mid X \in (-2\pi R, 2\pi R), \theta \in (0, \pi/2)\} \newline
\mathbb{E} &amp;= \{(X, \theta) \in \mathbb{R}^2 \mid X \in (-2\pi R, 2\pi R), \theta \in (0, \pi/6)\}
\end{aligned}
$$</p>
<p>Or&mdash;
$$
\begin{aligned}
\Omega &amp;= (-2\pi R, 2\pi R) \times (0, \pi/2) \newline
\mathbb{E} &amp;= (-2\pi R, 2\pi R) \times (0, \pi/6)
\end{aligned}
$$</p>
<p>We can now calculate the probability, like&mdash;
$$
\begin{aligned}
P(\mathbb{E}) &amp;= \frac{4\pi R \cdot (\pi/6)}{4\pi R \cdot (\pi/2)} = \frac{1}{3} \newline
P(\mathbb{E}) &amp;= \boxed{\frac{1}{3}}
\end{aligned}
$$</p>
<h3 id="method-3-radius-method">Method 3: Radius Method</h3>
<p>Take a chord passing through the center at an elevation $ \theta $ from the horizontal. Now we may consider all parallel chords to the diameter, at that elevation, their perpedicular distance from the center being $ x $. We get&mdash;</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-7.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>and the visualization&mdash;
<br>
<img src="/static/images/mathforquant/math-for-quant-part-1-8.svg" style="height: 300px !important"  class="invertible">
<br></p>
<p>The rest of it is elementary, and $ P(\mathbb{E}) $ comes out to&mdash;
$$
P(\mathbb{E}) = \boxed{\frac{1}{2}}
$$</p>
<h3 id="the-contradiction">The Contradiction</h3>
<p>So, we have <strong>Three answers</strong>. This is clearly a contradiction. It means somewhere along the way, our approach with <strong>highschool math</strong> was <strong>flawed</strong>. This problem is called the <strong>Bertrand Paradox</strong>. To correct this we would need an even deeper understanding of probability theory, beyond the typical highschool math.  <br />
In reality it is the question that may be misunderstood, the right interpretation is crucial to finding the solution to this paradox. We must realise that <strong>the choice of our random chord can impact the probability measure</strong>. There are <strong>different kinds of random</strong>.  </p>
<p>Unfortunately, we need the proper "advanced probability theory" to properly discuss the resolution, which is what we'll be exploring in this article. I might write a follow-up post for the resolution as this post is already going to get pretty long. If you really want to know the in-depth reason you can check out the <a href="https://en.wikipedia.org/wiki/Bertrand_paradox_(probability)">Wikipedia Page</a> <br />
(HINT: the answer, for uniform distributions, is $ \frac{1}{2} $)</p>
<h1 id="terms-and-definitions">Terms and Definitions</h1>
<p>Understanding the terms and notation is the most crucial part to understanding probability. </p>
<p>Let's try to intuitively find out what are the terms required to make a reasonable comment on the probability of an event.  </p>
<h2 id="what-is-probability">What is Probability?</h2>
<p>One could call it the <strong>likelihood or chance</strong>, of a certain event happening. (not to be confused with plausibility)    </p>
<p>What information would we need to comment on such a matter? Usually we would need to know what it is, whose outcome we are trying to determine&mdash; Is it a coin flip? a horse race? a cricket match?&mdash; This is is called the <strong>Random Experiment (RE)</strong>.    </p>
<p>Say we want to determine the probability that a grain of sand ends up exactly on top of a treasure chest, on a beach, after being washed away. Obviously we cannot make any reasonably accurate prediction of such an event. Why is that so? It is because we <strong>do not know</strong> the set of <strong>all possible outcomes</strong> of our random experiment (RE). So to make an accurate comment on the probability of an event, we must also know the set of all possible outcomes of our RE. This set is called the <strong>Sample Space</strong> ($ \Omega $)</p>
<h2 id="sample-space">Sample Space</h2>
<p>Now the sample space space could be something like&mdash;</p>
<p>$$
\Omega = \{Heads, Tails\}
$$</p>
<p>for a coin flip to something like&mdash;</p>
<p>$$
\Omega = \{1,2,3,4,5,6 \}
$$</p>
<p>for a dice roll.    </p>
<p>It may also be something complex like&mdash;</p>
<p>$$
\Omega = \{(x,y) \in \mathbb{R}^2 \mid 0 \leq x \leq 1, 0 \leq y \leq 1\}
$$</p>
<p>i.e any random point on a unit square. Even more complex &mdash;</p>
<p>$$
\Omega = \{(x,y) \in \mathbb{R}^2 \mid 0.25 \leq x^2 + y^2 \leq 1\}
$$</p>
<p>i.e any random point on a unit disc, that is atleast 0.5 units away from the center. It could also be visualized as a ring. </p>
<p>Finally, can you guess what this represents &mdash;</p>
<p>$$
\Omega = \{(x,y,z) \in \mathbb{R}^3 \mid x^2 + y^2 + z^2 = 1, z \geq 0, \sqrt{x^2 + y^2 + (z-1)^2} \leq 1\}
$$</p>
<p>Kind of scary isn't it?
( HINT: it's a sphere, with a special condition.)   </p>
<p>This should give an idea as to <em>why</em> we need a formal notation for something so intuitive. As we will see later, the theory builds on this and becomes powerful enough to over-take the intuitive approach.</p>
<h2 id="sigma-algebra">Sigma Algebra</h2>
<p>Typically this is part of <strong>measure theory</strong>, so we'll only cover what we absolutely need to know.</p>
<p>Again <a href="https://en.wikipedia.org/wiki/%CE%A3-algebra">Wikipedia</a> says:</p>
<blockquote>
<p>In formal terms, a $\sigma$-algebra on a set $ X $ is a nonempty collection $ \Sigma $ of subsets of $ X $ closed under complement, countable unions, and countable intersections. The ordered pair ( $ X $, $ \Sigma $) is called a measurable space.</p>
</blockquote>
<p>Too Complicated, Didn't Understand (TC;DU)</p>
<p>Let's start with the simple problem of trying to have complete knowledge of all events that can possibly occur. How do we find this? simple its just all possible subsets of our sample space $ \Omega $&mdash; But can we assign probabilities to <strong>all</strong> of these events? </p>
<p>In some cases, no, we can't.</p>
<p>To put it simply think of something like this&mdash;</p>
<p>we have $ \Omega = [0,1] $, now if we say that we want an event ($ E $) where the <strong>length of the interval is 0.5</strong>.  <br />
How many such subsets do you think there are? infinite. It could be $ [0.2, 0.7] $, $ [0.3, 0.8] $, and so on.  </p>
<p>Now whats $ P(E)_{[0.2 , 0.7]} $? 
Common sense would say its <strong>0.5</strong> (which is correct) since it covers exactly half of the sample space. But this is where intuition breaks down, for example let's try the <strong>vitali set</strong>&mdash;</p>
<p>(NOTE: In hindsight, the exploration coming up wasn't necessary, you may skip it if you like.)</p>
<p><br></p>
<hr>
<p><br></p>
<h3 id="vitali-set-deep-dive">Vitali Set (Deep Dive)</h3>
<p>Suppose we organize the numbers $(x,y)$ from $[0,1]$ into groups based on a set condition, i.e, $ x - y \in \mathbb{Q} $, the difference must be rational. So we could have $0.7 - 0.2 = 0.5$, in the same group, and $0.8 - 0.3 = 0.5$ in the same group.<br />
This splits $[0,1]$ into a <strong>uncountably many groups</strong>, for instance, one group might include $0.2, 0.7, 0.2 + 1/3, 0.7 + 1/3,$ and so on, as long as the differences are rational. </p>
<p>Suppose, we pick <em>exactly one</em> number from each group to create a set $ V $, called the Vitali set. The rule is that the numbers we pick must not differ by a rational number, because if they did (like 0.2 and 0.7), they'd be in the same group, and we're only allowed one number per group. So something like $ 0.2 $, and $ 1/\sqrt{2} $ etc. This is tricky because there are uncountably many groups, and we rely on the axiom of choice to ensure we can pick one number from each.    </p>
<p>So, we've got our Vitali set $ V $, a collection of numbers, one from each group, where no two numbers differ by a rational. Why does this cause trouble? Let's try to assign it a probability $ P(V) = a $. To check if this works, we shift $ V $ by all rational numbers in $ [0, 1] $, like 0, 0.1, 0.2, 0.3, and so on. For each rational $ q $, the set $ V + q $ (where we add $ q $ to each number in $ V $, wrapping around to stay in $ [0, 1] $) is a shifted copy of $ V $. These shifted sets are <strong>disjoint</strong>&mdash;they don't overlap. Why? If some number appeared in both $ V + q_1 $ and $ V + q_2 $, say $ x + q_1 = y + q_2 $, then $ x - y = q_2 - q_1 $, a rational number, which would mean $ x $ and $ y $ are in the same group, contradicting the fact that $ V $ has only one number per group.</p>
<p>The union of all these shifted sets $ V + q $, over all rational $ q \in [0, 1] $, covers all of $ [0, 1] $. Why? Every number in $ [0, 1] $ belongs to some group, and by shifting $ V $'s representative from each group by all possible rational numbers, we hit every number in every group. Since $ P([0, 1]) = 1 $, the probability of this union should be 1. By our probability rule of countable additivity, the probability of the union is the sum of the probabilities of these disjoint sets:</p>
<p>$$ P\left( \bigcup_{q} (V + q) \right) = \sum_{q} P(V + q) $$</p>
<p>Since shifting by a rational doesn't change the "size" of the set (in a uniform measure), each $ V + q $ should have the same probability as $ V $, so $ P(V + q) = a $. There are countably many rational numbers in $ [0, 1] $, so the sum is:</p>
<p>$$ \sum_{q} P(V + q) = \sum_{q} a $$</p>
<p>Now, we're stuck:</p>
<ul>
<li>If $ a &gt; 0 $, the sum $ a + a + \dots $ over countably many terms is infinite, which can't equal 1. That's a contradiction.</li>
<li>If $ a = 0 $, the sum is 0, which also can't equal 1. Another contradiction.</li>
</ul>
<p>This shows we <em>can't</em> assign a probability to $ V $ without breaking our probability rules. The Vitali set is <strong>non-measurable</strong>-it's too strange to fit into our probability framework. This is why we can't assign probabilities to <em>every</em> subset of $ \Omega = [0, 1] $. We need a <strong>sigma algebra</strong> to limit ourselves to subsets that play nice.
<br></p>
<hr>
<p><br>
A sigma algebra $ \mathcal{F} $ is our curated list of "good" subsets-events we can assign probabilities to consistently. It follows three rules:</p>
<ol>
<li><strong>The whole space is included</strong>: $ \Omega \in \mathcal{F} $. We need to say "something happens" with probability 1.</li>
<li><strong>Closed under complements</strong>: If $ A \in \mathcal{F} $, then $ \Omega \setminus A \in \mathcal{F} $ ($\Omega - A$). If "the number is in $ [0.2, 0.7] $" is an event, "the number is <em>not</em> in $ [0.2, 0.7] $" (i.e., $ [0, 0.2) \cup (0.7, 1] $) is too.</li>
<li><strong>Closed under countable unions</strong>: If $ A_1, A_2, \dots \in \mathcal{F} $, then $ \bigcup_{i=1}^\infty A_i \in \mathcal{F} $. This lets us combine events like $ [0.1, 0.2] \cup [0.3, 0.4] $.</li>
</ol>
<p>For $ \Omega = [0, 1] $, a sigma algebra includes nice subsets like intervals, their unions, and complements, but leaves out non-measurable sets like the Vitali set.</p>
<p>But for our intents and purposes, one may think of this as a <strong>power set</strong>, for simplicity.</p>
<h2 id="the-borel-sigma-algebra">The Borel Sigma Algebra</h2>
<p><strong>TL:DR</strong> It's just a sigma algebra but open intervals.</p>
<p>So, what's a <strong>Borel sigma algebra</strong>? It's the standard sigma algebra for continuous spaces like $ [0, 1] $, $ \mathbb{R} $, or $ \mathbb{R}^2 $, used in problems like the Bertrand paradox. Think of it as the ultimate collection of measurable subsets that makes probability calculations safe and practical.</p>
<p>The Borel sigma algebra, denoted $ \mathcal{B} $, starts with all <strong>open sets</strong> in your space. In $ \Omega = [0, 1] $, open sets are intervals like $ (0.2, 0.5) $ (not including endpoints). In $ \mathbb{R}^2 $, they're open disks, rectangles, or shapes with smooth boundaries. The Borel sigma algebra is the smallest sigma algebra that contains all these open sets, built by:</p>
<ol>
<li>Including all open sets.</li>
<li>Adding their complements (closed sets, like $ [0.2, 0.5] $).</li>
<li>Adding all countable unions and intersections of these sets.</li>
</ol>
<p>This creates a huge collection of measurable sets (called <strong>Borel Sets</strong>), including:</p>
<ul>
<li>Intervals: $ [0.2, 0.5] $, $ (0, 1) $, or single points like $ {0.42} $.</li>
<li>Unions of intervals: $ [0.1, 0.3] \cup [0.5, 0.7] $.</li>
</ul>
<h2 id="probability-measures-and-probability-spaces">Probability Measures and Probability Spaces</h2>
<h3 id="probability-measures">Probability Measures</h3>
<p>So we laid down most of the formal stuff, now to actually define our <strong>Probability measure</strong>, we may take a function&mdash;</p>
<p>$$
P: \mathcal{F} \to [0,1]
$$</p>
<p>From domain = $\mathcal{F}$, to codomain/range = $[0,1]$. Note this assigns probabilities to <strong>events in</strong> $ \mathcal{F} $(similar to what is covered in highschool), and <strong>is different from PMFs and PDFs</strong> covered later on. 
It must satisfy:    </p>
<ol>
<li>$P(\Omega) = 1$  </li>
<li>$P(\phi) = 0$</li>
<li><strong>Countable additivity</strong>: For a countable collection of <strong>disjoint</strong> sets, $A_1, A_2, \dots \in \mathcal{F}$, $$  P(\bigcup_{i=1}^\infty A_i) = \Sigma_{i=1}^\infty P(A_i) $$</li>
</ol>
<p>If all of this seems obvious, good. But in most cases it would be advisable to think of continuous probability as different from discrete probability.</p>
<h3 id="probability-spaces">Probability Spaces</h3>
<p>So from the terms we defined above, now we can finally define our "world", a probability space. Let's simply package all the relevant stuff into a triple of $(\Omega, \mathcal{F}, P)$.    </p>
<p>For example, for a (single) fair coin flip&mdash;</p>
<p>$$
\begin{aligned}
\Omega &amp;= \{H, T\} \newline
\mathcal{F} &amp;= \{\phi, \{H\}, \{T\}, \{H, T\}\} \newline
P&amp;(\{H\}) = 0.5 , P({\{T\}}) = 0.5
\end{aligned}
$$</p>
<h2 id="random-variables">Random Variables</h2>
<p>So we have a sample space, but doing calculations on it could be quite difficult, especially because we don't really have any set <strong>ordering</strong>. We typically want something in terms of <strong>numbers</strong> to do anything practical with it. So we introduce <strong>Random Variables (RVs)</strong>, that solve this problem.</p>
<p>Despite the name, a Random Variable is actually a <strong>function</strong>&mdash; <br />
$$
X: \Omega \to \mathbb{R}
$$</p>
<p>It assigns a <strong>Real Number</strong> ($\mathbb{R}$) to every outcome in the sample space ($\Omega$).  <br />
Now obviously we can have <strong>two types</strong> of random variables.    </p>
<h3 id="discrete-random-variables">Discrete Random Variables</h3>
<p>It usually takes countable values, for example let's take a dice rolled 3 times. Then&mdash;</p>
<p>$$
\Omega = \{(i,j,k): i,j,k \in {1,2,3,4,5,6}\}
$$</p>
<p>and suppose our random variable ($X$) is the number of 6s, rolled in the 3 tries&mdash;</p>
<p>$$
X: \Omega \to \{0,1,2,3\}
$$</p>
<p>and also&mdash;</p>
<p>$$
X(\omega \in \Omega) = \text{ number of sixes}
$$</p>
<p>More formally&mdash;</p>
<p>$$
X((i,j,k)) = \Sigma_{m\in\{i,j,k\}} 1_{\{6\}}(m)
$$</p>
<p>where $1_{\{6\}}(m)$ is an indicator function, defined as:    </p>
<p>$$
1_{\{6\}}(m) = 
\begin{cases} 
1 &amp; \text{if } m = 6, \newline
0 &amp; \text{if } m \neq 6
\end{cases}
$$</p>
<p>But anyways, there are many ways to define a RV.    </p>
<p>Then where is the $\sigma$-algebra ($\mathcal{F}$)? This is the question that confused me for a long time.<br />
Think of it something like this&mdash;<br />
In the dice roll example above, if we want the probability of the event that the RV takes the value $X=1$ (i.e $P(X=1)$)  <br />
We would want the probability of the event defined by the set&mdash;
$$
\{\omega \in \Omega: X(\omega) = 1\}
$$  </p>
<p>This looks some thing like $\{(2,6,4),(4,6,5) \dots\}$. Now this set must belong to $\mathcal{F}$, i.e, <br />
$$
\{\omega \in \Omega: X(\omega) = 1\} \in \mathcal{F}
$$</p>
<p>This ensures our event is <strong>measurable</strong> (refer to the Vitali set above for the counter example). But when applied to most real-world scenarios, one doesn't have to actively check for this statement to be true.  </p>
<p>Also note, here the $ X $ maps from $ \Omega $ to a <strong>Borel Set</strong>, in this case $\{0,1,2,3\}$</p>
<h3 id="continuous-random-variables">Continuous Random Variables</h3>
<p>More or less similar to the discrete type, except&mdash;</p>
<p>$$
X: \Omega \to \mathbb{R}^n \text{ Where }  n \in \mathbb{Z}
$$</p>
<p>The rest can be inferred. Maybe try to guess what the RV was in the Bertrand Paradox stated earlier.</p>
<h1 id="probability-distributions">Probability distributions</h1>
<h2 id="the-pmf-and-the-pdf">The PMF and the PDF</h2>
<p>So, we've defined random variables, which turn outcomes in $\Omega$ into numbers, and we've set up our probability space ($\Omega, \mathcal{F}, P$) to assign probabilities to events. But how do we figure out the likelihood of a random variable taking specific values, like "exactly one 6 in three dice rolls" or "the chord's midpoint in the inner circle"? 
This is where <strong>probability distributions</strong> come in&mdash;they describe how probabilities are spread across the values of a random variable.    </p>
<p>The <strong>probability measure</strong> $ P $, assigns probabilities to <strong>events</strong> in $ \mathcal{F} $, but as we've seen before RVs aren't events. So just to compute the <strong>probability of the subset of events the RV takes</strong> directly (like $P(X=1)$), we introduce tools like the <strong>PMF and PDF</strong>.</p>
<h3 id="probability-mass-function-pmf">Probability Mass Function (PMF)</h3>
<p>For a <strong>discrete random variable</strong>, which takes countable values (like the number of 6s in three dice rolls), the probability mass function (PMF) gives the probability of each possible value: </p>
<p>$$
P(X = x)
$$  </p>
<p>Let's revisit our dice example. The sample space is&mdash;</p>
<p>$$
\Omega = \{(i,j,k): i,j,k \in {1,2,3,4,5,6}\}
$$</p>
<p>And $X((i,j,k)) = \Sigma_{m\in\{i,j,k\}} 1_{\{6\}}(m)$, where $1_{\{6\}}(m)$ is&mdash;    </p>
<p>$$
1_{\{6\}}(m) = 
\begin{cases} 
1 &amp; \text{if } m = 6, \newline
0 &amp; \text{if } m \neq 6
\end{cases}
$$</p>
<p>Here the PMF tells us probabilities like&mdash; $ P(X=1) $, i.e only one 6 is rolled, in three tries. Now for simple stuff like this dice roll, even highschool math will suffice, but just to put it out there, $X$ follows a <strong>binomial distribution</strong> (Will be covered ahead). So the PMF looks something like&mdash; </p>
<p>$$
P(X=k) = \binom{3}{k}\cdot\left(\frac{1}{6}\right)^k\cdot\left(\frac{5}{6}\right)^{3-k}  \text{ where }   k \in \{0,1,2,3\} 
$$</p>
<p>Here $k = 1$ gives&mdash;</p>
<p>$$
P(X=1) = \binom{3}{1}\cdot\left(\frac{1}{6}\right)^1\cdot\left(\frac{5}{6}\right)^{2} = \frac{75}{216} \approx 0.347
$$  </p>
<h3 id="probability-density-function-pdf">Probability Density Function (PDF)</h3>
<p>For a <strong>continuous random variable</strong>, which takes values in a continuum (like the coordinates of a chord's midpoint in the Bertrand paradox), we can't assign probabilities to specific values because $ P(X = x) = 0 $. So what, do we do? Is it just not possible? Of course not! We take an interval of values, like $ P(a \leq X \leq b) $. This should give us the probability that $ X $ lies in the interval $ [a , b] $, which usually won't be zero (To serve as an abstraction, one can think of this as taking a <strong>small section</strong> of a line, rather than a <strong>point</strong> on a line. The point will have no length on the line, which makes defining probabilities for it quite difficult, but the small section has some finite length, however small it may be).   </p>
<p>How do we do this though? Simple. We define another function, called the <strong>probability density function (PDF)</strong> ( $ f(x) $ ). We want this function to be some kind of a "density map" of sorts.    </p>
<p>So obviously to find the probability of a specific part we'd do&mdash;  </p>
<p>$$
P(a \leq X \leq b) = \frac{\int_{a}^{b} f(x)dx}{\int_{-\infty}^{\infty} f(x)dx}
$$</p>
<p>But that does not look pretty, especially since $ f(x) $ is not <strong>normalized</strong>. So we conviniently define $ f(x) $ in such a way that&mdash;    </p>
<p>$$
\int_{-\infty}^{\infty} f(x)dx = 1
$$</p>
<p>So now we get&mdash;</p>
<p>$$
P(a \leq X \leq b) = \int_{a}^{b} f(x)dx
$$</p>
<p>Also it must be <strong>non-negative</strong> for all $x$. i.e $f(x)\geq 0 \forall x$ (try to justify this claim, it's pretty intuitive, but the proof is still quite long and excluded for brevity).    </p>
<p>Let's do some examples because abstract definitions like this make no sense.    </p>
<hr />
<h4 id="example-1-uniform-distribution-on-an-interval-1d">Example 1: Uniform Distribution on an Interval (1D)</h4>
<p><strong>Scenario</strong>: Suppose you're waiting for a bus that arrives randomly between 0 and 10 minutes from now. Let $X$ be the waiting time in minutes, uniformly distributed over $[0, 10]$.</p>
<p><strong>PDF</strong>: Since $X$ is equally likely to take any value in $[0, 10]$, the PDF is constant over that interval. The total area under the PDF must be 1:</p>
<p>$$
f(x) = \frac{1}{10}, \quad 0 \leq x \leq 10
$$</p>
<p>(Outside $[0, 10]$, $f(x) = 0$.) Check: $\int_0^{10} \frac{1}{10} \, dx = \frac{10}{10} = 1$.</p>
<p><strong>Probability</strong>: What's the probability you wait between 2 and 5 minutes ($P(2 \leq X \leq 5)$)?</p>
<p>$$
P(2 \leq X \leq 5) = \int_2^5 \frac{1}{10} \, dx = \frac{5 - 2}{10} = \frac{3}{10} = 0.3
$$</p>
<p><strong>Intuition</strong>: The PDF is a flat line at height $\frac{1}{10}$. The interval $[2, 5]$ has length 3, so the area is $3 \cdot \frac{1}{10} = 0.3$.</p>
<p>It looks something like&mdash;</p>
<p><br></p>
<p><img src="/static/images/mathforquant/math-for-quant-part-1-9.svg" style="height: 300px !important" class="invertible"></p>
<p><br>
<br></p>
<hr />
<p><br>
For the sake of brevity, we'll go straight to the named distributions from here, but maybe try to think up a few more examples, before we do. Maybe try to get an exponential distribution (i.e in terms of $e^k \text{ or } a^k$)? (Remember you must normalize them in some way to get $f(x)$)</p>
<h2 id="the-binomial-distribution">The Binomial distribution</h2>
<p>We've already seen the binomial distribution sneak into our dice example, where we calculated the probability of rolling exactly one 6 in three tries. But what exactly is this distribution, and why is it so important? Let's break it down.  </p>
<p>Let's start with the main question:</p>
<blockquote>
<p>Given a fixed number of trials (<code>n</code>), flipping a fair coin, calculate the probability of getting exactly 1 heads in these <code>n</code> trials.</p>
</blockquote>
<p>Simple high school math right?<br />
But let's visualize it&mdash;</p>
<p><br>
<img src="/static/images/mathforquant/math-for-quant-part-1-10.svg" style="height: 300px !important" class="invertible">
<br></p>
<p>Notice that if we select a path all the way down to the bottom, the probabilities of each branch get <strong>multiplied</strong>. In our case, as it's a fair coin, $ p = (1-p) = 0.5 $.
here we <strong>only need 1 heads</strong>, so we must continue on the rightmost branch, and select the place where we get our heads. We can select the "branch" where we make the split with&mdash;</p>
<p>$$
\binom{n}{1} = n
$$</p>
<p>And our final probability will look like&mdash;</p>
<p>$$
\binom{n}{1}\cdot(0.5)^1\cdot(0.5)^{(n-1)}
$$</p>
<p>So to generalize this:<br />
$$
\begin{aligned}
p:&amp; \text{ Probability of success.} \newline
n:&amp; \text{ Number of trials} \newline
k:&amp; \text{ Required number of successes}
\end{aligned}
$$</p>
<p>So we get a RV, $X$, and&mdash;
$$
P(X=k) = \binom{n}{k}\cdot(p)^k\cdot(1-p)^{(n-k)}
$$
&mdash;Is the PMF. Here, $\binom{n}{k}$ counts the ways to choose $k$ successes out of $n$ trials, $p^k$ is the probability of $k$ successes, and $(1 - p)^{n - k}$ covers the failures.
Here is the graph ($n=10, p=0.5$)&mdash;
<br></p>
<iframe src="https://www.desmos.com/calculator/uwq5nondpt?embed" width="100%"  height="500" style="border: 1px solid #ccc;border-radius:5px" frameborder=0></iframe>
<p><br></p>
<p>The green lines are the floored values of $X$, i.e Integers, where as the red curve is the extension of this binomial distribution. Can you point out what's wrong with it? yeah. The red graph may cause confusion, as the binomial distribution is <strong>discrete</strong> in nature, so it shouldn't have a continuous graph. So the real binomial distribution is the <strong>green lines only</strong>, or more accurately, only the points corresponding to integral values of $X$.   </p>
<p>By the way, if $X$ <strong>follows a Binomial distribution</strong>, we say $ X \sim \text{Binomial}(n, p) $, in math language. Now the fun part&mdash;</p>
<h3 id="mean-expected-value">Mean (Expected Value)</h3>
<p>We want the expected value, $E[X]$, which is like the average number of successes we'd expect over many, many trials. The probability of getting exactly $k$ successes is given by the binomial probability mass function (PMF):</p>
<p>$$
P(X = k) = \binom{n}{k} \cdot p^k \cdot (1 - p)^{n - k}
$$</p>
<p>To find $E[X]$, we weigh each possible number of successes $k$ by its probability:</p>
<p>$$
E[X] = \sum_{k=0}^n k \cdot \binom{n}{k} \cdot p^k \cdot (1 - p)^{n - k}
$$</p>
<p>Notice that when $k = 0$, the term is $0 \cdot P(X = 0) = 0$, so it contributes nothing. Let's skip it and start from $k = 1$:</p>
<p>$$
E[X] = \sum_{k=1}^n k \cdot \binom{n}{k} \cdot p^k \cdot (1 - p)^{n - k}
$$</p>
<p>Now, that $k \cdot \binom{n}{k}$ looks a bit tricky. Let's play with it. There's a neat identity, taught in highschool, that can be used here:</p>
<p>$$k \cdot \binom{n}{k} = n \cdot \binom{n-1}{k-1}$$</p>
<p>Let's actually use it:</p>
<p>$$
E[X] = \sum_{k=1}^n \left[ n \cdot \binom{n-1}{k-1} \right] \cdot p^k \cdot (1 - p)^{n - k} = n \sum_{k=1}^n \binom{n-1}{k-1} \cdot p^k \cdot (1 - p)^{n - k}
$$</p>
<p>To make this sum friendlier, let's shift the index. Set $j = k - 1$. When $k = 1$, $j = 0$; when $k = n$, $j = n-1$. Also, adjust the exponents: $p^k = p^{j+1} = p \cdot p^j$ and $(1 - p)^{n - k} = (1 - p)^{(n-1) - j}$. So the sum becomes:</p>
<p>$$
E[X] = n \sum_{j=0}^{n-1} \binom{n-1}{j} \cdot (p \cdot p^j) \cdot (1 - p)^{(n-1) - j} = n p \sum_{j=0}^{n-1} \binom{n-1}{j} \cdot p^j \cdot (1 - p)^{(n-1) - j}
$$</p>
<p>That sum looks familiar&mdash;it's the total probability of a binomial random variable with $n-1$ trials and probability $p$, which sums to 1 (since it's the sum of all probabilities for a $\text{Binomial}(n-1, p)$ distribution). So:</p>
<p>$$
\sum_{j=0}^{n-1} \binom{n-1}{j} \cdot p^j \cdot (1 - p)^{(n-1) - j} = 1
$$</p>
<p>Thus:</p>
<p>$$
E[X] = n p \cdot 1 = n p
$$</p>
<p>$$
\boxed{E[X] = n p}
$$</p>
<p>That looks like something we'd expect, right? If you flip a coin $n$ times with success probability $p$, you expect $n p$ successes on average. Like, if you flip a fair coin ($p = 0.5$) 10 times, you'd expect about 5 heads.</p>
<h3 id="variance">Variance</h3>
<p>Now for the variance, $\text{Var}(X) = E[X^2] - (E[X])^2$, which tells us how spread out our number of successes is. Calculating $E[X^2]$ directly is a bit messy, so let's be clever and compute $E[X(X-1)]$ first, which is like looking at pairs of successes. Then we'll relate it to $E[X^2]$. Start with:</p>
<p>$$
E[X(X-1)] = \sum_{k=0}^n k (k-1) \cdot \binom{n}{k} \cdot p^k \cdot (1 - p)^{n - k}
$$</p>
<p>When $k = 0$ or $k = 1$, $k(k-1) = 0$, so those terms vanish. Start at $k = 2$:</p>
<p>$$
E[X(X-1)] = \sum_{k=2}^n k (k-1) \cdot \binom{n}{k} \cdot p^k \cdot (1 - p)^{n - k}
$$</p>
<p>Another cool identity: $k (k-1) \cdot \binom{n}{k} = n (n-1) \cdot \binom{n-2}{k-2}$. (Just kidding, its the same thing applied twice)
$$
\begin{aligned}
E[X(X-1)] =&amp; \sum_{k=2}^n \left[ n (n-1) \cdot \binom{n-2}{k-2} \right] \cdot p^k \cdot (1 - p)^{n - k} \newline  <br />
=&amp; n (n-1) \sum_{k=2}^n \binom{n-2}{k-2} \cdot p^k \cdot (1 - p)^{n - k}
\end{aligned}
$$</p>
<p>Reindex with $j = k - 2$. When $k = 2$, $j = 0$; when $k = n$, $j = n-2$. Exponents adjust: $p^k = p^{j+2} = p^2 \cdot p^j$, and $(1 - p)^{n - k} = (1 - p)^{(n-2) - j}$. So:</p>
<p>$$
E[X(X-1)] = n (n-1) p^2 \sum_{j=0}^{n-2} \binom{n-2}{j} \cdot p^j \cdot (1 - p)^{(n-2) - j}
$$</p>
<p>That sum is the total probability for a $\text{Binomial}(n-2, p)$, which equals 1:</p>
<p>$$
E[X(X-1)] = n (n-1) p^2 \cdot 1 = n (n-1) p^2
$$</p>
<p>Now, relate $E[X(X-1)]$ to $E[X^2]$:</p>
<p>$$
E[X(X-1)] = E[X^2 - X] = E[X^2] - E[X]
$$</p>
<p>So:</p>
<p>$$
E[X^2] = E[X(X-1)] + E[X] = n (n-1) p^2 + n p
$$</p>
<p>Now compute the variance:</p>
<p>$$
\text{Var}(X) = E[X^2] - (E[X])^2 = \left[ n (n-1) p^2 + n p \right] - (n p)^2
$$</p>
<p>Simplify:</p>
<p>$$
= n (n-1) p^2 + n p - n^2 p^2 = n^2 p^2 - n p^2 + n p - n^2 p^2 = n p - n p^2 = n p (1 - p)
$$</p>
<p>$$
\boxed{\text{Var}(X) = n p (1 - p)}
$$</p>
<p>Interesting! The variance $n p (1 - p)$ is highest when $p = 0.5$, meaning the outcome is most uncertain when success and failure are equally likely. For $p$ close to 0 or 1, the variance shrinks, as the outcome is more predictable.</p>
<p>So, our final answers are:</p>
<p>$$
\begin{aligned}
E[X] &amp;= \boxed{n p} \newline
\text{Var}(X) &amp;= \boxed{n p (1 - p)}
\end{aligned}
$$</p>
<h2 id="the-poisson-distribution">The Poisson distribution</h2>
<p>Imagine we're running a website, and we're interested in the number of visitors arriving in a given hour, and possibly predict average traffic and future probabilities of extreme traffic with it. (for e.g how likely 10k visitors are, etc)  <br />
How do we model this, with the math we've learnt? How about we make every second a RV ($X$)? We'd have two possibilities:   </p>
<ol>
<li>A user visits the site (success)</li>
<li>No one visits the site (failure) </li>
</ol>
<p>Looks familiar? That's because $X$ follows a <strong>binomial distribution</strong>! But clearly anyone can see the limitations here&mdash;  </p>
<ol>
<li>What if two users visit in 1 second.</li>
<li>Computationally infeasible. (we're evaluating the binomial formula. every. single. second. i.e 3600 times an hour, or even higher for finer precision)</li>
<li>$P(\text{success})$ is tiny.</li>
</ol>
<h3 id="deriving-the-poisson-from-the-binomial">Deriving the Poisson from the Binomial</h3>
<p>So instead, how about we calculate the <strong>average rate</strong> of visitors (the mean), say $\lambda$, and try to do something with that&mdash;   <br />
But what exactly? We mainly want to model the probability of getting $k$ (5 or 10 or 20) visitors an hour, simply from the fact that we know that on average there are $\lambda$ visitors an hour. Let's start with the binomial version&mdash; </p>
<p>$$
P_{\text{binomial}}(X=k) = \binom{n}{k}\cdot(p)^{k}\cdot(1-p)^{n-k}
$$  </p>
<p>But now our no. of trials $n \to \infty$ , and our probability of success $p \to 0$. With limits, that'd look something like&mdash; </p>
<p>$$
P_{\text{poisson}}(X=k) = \lim_{n \to \infty ;\text{  } p \to 0} \binom{n}{k}\cdot(p)^{k}\cdot(1-p)^{n-k}
$$  </p>
<p>Now even writing it out looks, funky, so we must find a way to reduce this to <strong>one variable</strong>. Recall that the mean of a binomial was $np$. But we set the average rate of visitors to $\lambda$, so&mdash;</p>
<p>$$
\begin{aligned}
np &amp;= \lambda \newline
p &amp;= \frac{\lambda}{n}
\end{aligned}
$$</p>
<p>Now let's put this back in that limit, so&mdash;</p>
<p>$$
P_{\text{poisson}}(X=k) = \lim_{n \to \infty} \binom{n}{k}\cdot\left(\frac{\lambda}{n}\right)^{k}\cdot\left(1-\frac{\lambda}{n}\right)^{n-k}
$$  </p>
<p>Looks a bit easier now... Oh, let's expand the combination to it's factorial notation&mdash;</p>
<p>$$
P_{\text{poisson}}(X=k) = \lim_{n \to \infty} \left(\frac{n!}{k!(n-k)!} \right) \cdot \left(\frac{\lambda}{n}\right)^{k} \cdot \left(1-\frac{\lambda}{n}\right)^{n-k}
$$  </p>
<p>So we have three terms we have to deal with&mdash;</p>
<p>$$
\begin{aligned}
\lim_{n \to \infty} &amp;\left(\frac{n!}{k!(n-k)!} \right) \newline
\lim_{n \to \infty} &amp;\left(\frac{\lambda}{n}\right)^{k} \newline
\lim_{n \to \infty} &amp;\left(1-\frac{\lambda}{n}\right)^{n-k}
\end{aligned}
$$</p>
<p>Expand the first one&mdash;</p>
<p>$$
\lim_{n \to \infty} \left(\frac{n(n-1)(n-2) \dots (n-k+1)\cdot(n-k)(n-k-1)\dots}{k!(n-k)(n-k-1)\dots} \right) 
$$</p>
<p>Notice how everything after the dot gets cancelled with the denominator? But also notice, that $n \to \infty$, so $n \approx (n-1) \approx (n-k+1)$. Yes, there is a proper mathematical way to do it, but let's just keep it intuitive and move on. So now our expression is&mdash;</p>
<p>$$
\lim_{n \to \infty} \left(\frac{n^k}{k!} \right) 
$$  </p>
<p>Do you see it? Let's bring back the second part&mdash;</p>
<p>$$
\lim_{n \to \infty} \left(\frac{n^k}{k!} \right)\cdot \left(\frac{\lambda}{n}\right)^{k}
$$</p>
<p>It gets cancelled! So together we're only left with normal variables for the first and second part, i.e &mdash;
$$
\lim_{n \to \infty} \left(\frac{\lambda^k}{k!} \right)
$$</p>
<p>For the last part, we have to get a little creative (although it's a simple trick taught in JEE prep. and stuff). Let $L$ be the limit for the 3rd part.  </p>
<p>$$
L = \lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^{n-k}
$$</p>
<p>Taking <strong>natural log</strong> on both sides we get&mdash;</p>
<p>$$
\ln(L) = \lim_{n \to \infty}  (n-k)\ln\left(1-\frac{\lambda}{n}\right)
$$</p>
<p>Of course, there are a few nuances to this step, but let's not bother. Rewriting it to get $0/0$ form&mdash;</p>
<p>$$
\ln(L) = \lim_{n \to \infty}  \frac{\ln\left(1-\frac{\lambda}{n}\right)}{\frac{1}{(n-k)}}
$$</p>
<p>What do we do next? L'Hopital of course, because we're lazy&mdash;</p>
<p>$$
\ln(L) = \lim_{n \to \infty}  \frac{\frac{1}{1-\frac{\lambda}{n}}\cdot\frac{\lambda}{n^2}}{\frac{-1}{(n-k)^2}}
$$</p>
<p>Hm&mdash; $(n-k)^2 \approx n^2$ so let's just cancel them, and put $\lambda / n = 0$. And with that we've got rid of the limit&mdash;</p>
<p>$$
\ln(L) = -\lambda
$$</p>
<p>So&mdash;
$$
L = e^{-\lambda}
$$</p>
<p>Put it all back together&mdash;
$$
\boxed{P_{\text{poisson}}(X=k) = \frac{\lambda^k \cdot e^{-\lambda}}{k!}}
$$</p>
<p>Where, $X \sim \text{Poisson}(\lambda) $</p>
<p>And here is the graph($\lambda = 2$) &mdash;</p>
<p><br>
<br></p>
<iframe src="https://www.desmos.com/calculator/tirmhififj?embed"width="100%"  height="500" style="border: 1px solid #ccc;border-radius:5px" frameborder=0></iframe>
<p><br>
<br></p>
<p>Of course the red graph is wrong, and the green one is the true graph (floored). Notice something peculiar? Getting $\lambda - 1$ has the same probability as getting $ \lambda $ visitors (when $\lambda$ is an integer, which it always is in our case).</p>
<h3 id="mean-expected-value-variance">Mean (Expected Value) &amp; Variance</h3>
<p>To save time, I'll just say it (Feel free to derive it on your own, it's not that hard). The mean and variance are&mdash;</p>
<p>$$
\begin{aligned}
E[X] &amp;= \lambda \newline
\text{Var}(X) &amp;= \lambda
\end{aligned}
$$</p>
<p>The mean is understandable because that's what we assumed, but <strong>why does the variance depend on the mean?</strong>  <br />
What does our expected value, have to do with spread? Think about it.</p>
<h2 id="the-gaussian-normal-distribution">The Gaussian (Normal) distribution</h2>
<p>Take a look at this mark distribution for JEE:</p>
<p><img src="/static/images/mathforquant/math-for-quant-part-1-11.png" style="height: 300px !important" class="invertible"></p>
<p><br>
Or this marathon finish-time distribution&mdash;</p>
<p><a href="https://www.researchgate.net/figure/Histogram-showing-the-finishing-times-of-all-40-year-old-runners-The-curve-represents-a_fig6_277089242"><img src="https://www.researchgate.net/profile/Niklas-Lehto/publication/277089242/figure/fig6/AS:269869983727616@1441353426784/Histogram-showing-the-finishing-times-of-all-40-year-old-runners-The-curve-represents-a.png" alt="Histogram showing the finishing times of all 40-year-old runners. The curve represents a fit of a normal distribution to the measured distribution."/></a></p>
<p><br>
Or stock prices (random walk) over a long time&mdash;
<a href="https://stats.stackexchange.com/questions/159650/why-does-the-variance-of-the-random-walk-increase"><img src="/static/images/mathforquant/math-for-quant-part-1-12.png"/></a></p>
<p><br></p>
<p>Did you spot it? exactly all of them look kind of "bell-shaped". That's called a normal/gaussian distribution&mdash;  <br />
But what exactly is the question it is trying to answer? It tries to answer this question:   </p>
<blockquote>
<p>What's the likelihood that a continuous random variable, shaped by many small, independent random effects, falls within a specific range?</p>
</blockquote>
<p>This applies to scenarios like test scores, where individual strengths and weaknesses average out, or stock prices, where daily fluctuations accumulate. Unlike the binomial (discrete trials) or Poisson (rare events), the Gaussian handles <strong>continuous</strong> data with a characteristic bell shape, which turns out to be pretty useful in real-world applications, beyond just recreational mathematics.</p>
<h3 id="deriving-the-gaussian-from-the-binomial">Deriving the Gaussian from the Binomial</h3>
<p>(NOTE: Heavy math incoming, it scares me too. It's better to derive it by yourself, which also makes it easier to understand. When starting out, it's better to just memorize the formula and learn to apply it before getting into this derivation, as it involves a lot of complicated approximations)</p>
<p>The binomial distribution models the number of successes in $n$ trials, each with success probability $p$. Suppose we're flipping a fair coin ($p = 0.5$) many times, say $n = 1000$, and we want the probability of getting $k$ heads. The PMF is:</p>
<p>$$
P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}
$$</p>
<p>For large $n$, computing $\binom{n}{k}$ directly is a nightmare. Plus, the distribution starts to look smooth, almost like a continuous curve (remember the green lines in the binomial graph?). Let's explore what happens when $n \to \infty$.    </p>
<p>(NOTE: Unlike the poisson distribution, here $p$ does not tend to 0)</p>
<p>First, let's "center" the distribution. The mean of a binomial is $E[X] = n p$, and the variance is $\text{Var}(X) = n p (1 - p)$. For $p = 0.5$, the mean is $n/2$, and the standard deviation is $\sqrt{n \cdot 0.5 \cdot 0.5} = \sqrt{n/4} = \sqrt{n}/2$. Let's <strong>standardize</strong> the random variable $X$ to see its behavior:</p>
<p>$$
Z = \frac{X - n p}{\sqrt{n p (1 - p)}}
$$</p>
<p>Wait, what exactly did we do? why "standardize" it?   <br />
Remember how we normalized continuous RVs? exactly. We'd prefer a more standardized way to represent the same distribution, which would make it easier to compare to other such distributions. So we ask it: "<strong>How far is $ X $ from its average, relative to its typical spread?</strong>".  </p>
<p>Here, $Z$ measures how many standard deviations $X$ is from the mean, which answers the first question.    </p>
<p>Now, We want the probability density of $Z$ as $n$ grows large. To do this, let's approximate $P(X = k)$ for large $n$ using <a href="https://en.wikipedia.org/wiki/Stirling%27s_approximation"><strong>Stirling's approximation</strong></a>, which simplifies factorials:</p>
<p>$$
n! \approx \sqrt{2 \pi n} \left( \frac{n}{e} \right)^n
$$</p>
<p>For the binomial coefficient:</p>
<p>$$
\binom{n}{k} = \frac{n!}{k! (n - k)!} \approx \frac{\sqrt{2 \pi n} \left( \frac{n}{e} \right)^n}{\sqrt{2 \pi k} \left( \frac{k}{e} \right)^k \sqrt{2 \pi (n - k)} \left( \frac{n - k}{e} \right)^{n - k}}
$$</p>
<p>Simplify:</p>
<p>$$
\binom{n}{k} \approx \frac{\sqrt{2 \pi n}}{\sqrt{2 \pi k} \sqrt{2 \pi (n - k)}} \cdot \frac{\frac{n^n}{e^n}}{\frac{k^k (n - k)^{n - k}}{e^k e^{n - k}}} = \sqrt{\frac{n}{2 \pi k (n - k)}} \cdot \frac{n^n}{k^k (n - k)^{n - k}}
$$</p>
<p>Now, the binomial PMF is:</p>
<p>$$
P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k} \approx \sqrt{\frac{n}{2 \pi k (n - k)}} \cdot \frac{n^n}{k^k (n - k)^{n - k}} \cdot p^k (1 - p)^{n - k}
$$</p>
<p>Let's express $k$ around the mean: set $k = n p + x \sqrt{n p (1 - p)}$, so $x$ represents deviations in standard deviation units (like $Z$). Then:</p>
<p>$$
n - k = n - (n p + x \sqrt{n p (1 - p)}) = n (1 - p) - x \sqrt{n p (1 - p)}
$$</p>
<p>For large $n$, assume $k \approx n p$, so $k (n - k) \approx (n p) (n (1 - p)) = n^2 p (1 - p)$. The square root term becomes:</p>
<p>$$
\sqrt{\frac{n}{2 \pi k (n - k)}} \approx \sqrt{\frac{n}{2 \pi n^2 p (1 - p)}} = \frac{1}{\sqrt{2 \pi n p (1 - p)}}
$$</p>
<p>Now tackle the rest. The term $\frac{n^n}{k^k (n - k)^{n - k}} p^k (1 - p)^{n - k}$ needs careful handling. Take its logarithm to simplify:</p>
<p>$$
\ln \left( \frac{n^n p^k (1 - p)^{n - k}}{k^k (n - k)^{n - k}} \right) = n \ln n + k \ln p + (n - k) \ln (1 - p) - k \ln k - (n - k) \ln (n - k)
$$</p>
<p>(NOTE: After this, writing out the rest of it will simply look messy. So it's better to do it by oneself.)  </p>
<p>Substitute $k = n p + x \sqrt{n p (1 - p)}$ and use Taylor expansions around the mean. This gets messy, so let's focus on the exponent. For large $n$, the binomial PMF approximates a density. After substituting and simplifying (using normal approximation techniques), we get:</p>
<p>$$
\boxed{P(X = k) \approx \frac{1}{\sqrt{2 \pi n p (1 - p)}} e^{-\frac{(k - n p)^2}{2 n p (1 - p)}}}
$$</p>
<p>For the standardized variable $Z$, the density of $Z \approx x$ is:</p>
<p>$$
f(z) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{z^2}{2}}
$$</p>
<p>This is the <strong>standard normal distribution</strong>, $Z \sim \mathcal{N}(0, 1)$, with mean 0 and variance 1. For a general normal distribution $X \sim \mathcal{N}(\mu, \sigma^2)$, where $\mu$ is the mean and $\sigma^2$ is the variance, we transform $Z = \frac{X - \mu}{\sigma}$, so the PDF of $X$ is:</p>
<p>$$
\boxed{f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}}}
$$</p>
<p>Let's verify it integrates to 1:</p>
<p>$$
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}} dx
$$</p>
<p>Substitute $u = \frac{x - \mu}{\sigma}$, so $x = \mu + \sigma u$, $dx = \sigma du$:</p>
<p>$$
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{u^2}{2}} \sigma du = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{-\frac{u^2}{2}} du = 1
$$</p>
<p>(The integral equals $\sqrt{2 \pi}$, so it normalizes correctly.) The PDF is non-negative, satisfying all requirements.</p>
<p>Here's the graph for $\mathcal{N}(0, 1)$:</p>
<p><br></p>
<iframe src="https://www.desmos.com/calculator/kdgkdzq2qx?embed" width="100%" height="500" style="border: 1px solid #ccc;border-radius:5px" frameborder=0></iframe>
<p><br></p>
<h3 id="mean-and-variance">Mean and Variance</h3>
<p>The mean of $X \sim \mathcal{N}(\mu, \sigma^2)$ is:</p>
<p>$$
E[X] = \int_{-\infty}^{\infty} x \cdot \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}} dx
$$</p>
<p>Substitute $u = \frac{x - \mu}{\sigma}$, so $x = \mu + \sigma u$, $dx = \sigma du$:</p>
<p>$$
E[X] = \int_{-\infty}^{\infty} (\mu + \sigma u) \cdot \frac{1}{\sqrt{2 \pi}} e^{-\frac{u^2}{2}} du = \mu \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{-\frac{u^2}{2}} du + \sigma \int_{-\infty}^{\infty} u \cdot \frac{1}{\sqrt{2 \pi}} e^{-\frac{u^2}{2}} du
$$</p>
<p>The first integral is 1 (total probability). The second is 0 (since $u e^{-\frac{u^2}{2}}$ is odd). So:</p>
<p>$$
E[X] = \mu
$$</p>
<p>For variance, compute $E[(X - \mu)^2]$:</p>
<p>$$
\text{Var}(X) = \int_{-\infty}^{\infty} (x - \mu)^2 \cdot \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}} dx
$$</p>
<p>Using $u = \frac{x - \mu}{\sigma}$:</p>
<p>$$
\text{Var}(X) = \int_{-\infty}^{\infty} (\sigma u)^2 \cdot \frac{1}{\sqrt{2 \pi}} e^{-\frac{u^2}{2}} du = \sigma^2 \int_{-\infty}^{\infty} u^2 \cdot \frac{1}{\sqrt{2 \pi}} e^{-\frac{u^2}{2}} du
$$</p>
<p>The integral is the variance of a standard normal, which is 1 (by definition or computation). Thus:</p>
<p>$$
\text{Var}(X) = \sigma^2
$$</p>
<p>So:</p>
<p>$$
\begin{aligned}
E[X] &amp;= \boxed{\mu} \newline
\text{Var}(X) &amp;= \boxed{\sigma^2}
\end{aligned}
$$</p>
<h1 id="conclusion">Conclusion</h1>
<h2 id="recap">Recap</h2>
<p>This blog is not a formal learning resource but a personal account of my journey exploring advanced probability as a high school student, not a professional. It extends beyond high school basics into concepts essential for financial mathematics and statistics. The <strong>Bertrand Paradox</strong> illustrates the challenges of defining randomness, producing conflicting probabilities ($ 1/4 $, $ 1/3 $, $ 1/2 $) for a chord's length depending on the selection method, highlighting the need for a rigorous framework. This post does not resolve the paradox due to its length, but the concepts introduced provide tools to tackle it, with a follow-up post planned for the future.</p>
<p>Key concepts covered include:    </p>
<ul>
<li><strong>Sample Space ($ \Omega $)</strong>: The set of all possible outcomes of a random experiment.</li>
<li><strong>Sigma Algebra ($ \mathcal{F} $)</strong>: A collection of measurable subsets of $ \Omega $, closed under complement and countable unions, ensuring consistent probability assignments (e.g., excluding non-measurable Vitali sets).</li>
<li><strong>Probability Measure ($ P $)</strong>: A function that maps events in $ \mathcal{F} $ to $[0,1]$, satisfying $ P(\Omega) = 1 $ and countable additivity.</li>
<li><strong>Probability Space ($ (\Omega, \mathcal{F}, P) $)</strong>: The triple defining the sequence.</li>
<li>
<p><strong>Random Variables</strong>: Functions $ X: \Omega \to \mathbb{R} $, either discrete (countable values) or continuous (continuum values), with probabilities assigned to events in $ \mathcal{F} $.  </p>
</li>
<li>
<p><strong>Probability Distributions</strong>:  </p>
<ol>
<li><strong>Binomial Distribution</strong>: Models $ k $ successes in $ n $ trials, with PMF<br />
  $$
  \begin{aligned}
  P(X = k) &amp;= \binom{n}{k} p^k (1-p)^{n-k},\newline
  \text{mean } E[X] &amp;= np,\newline
  \quad \text{variance } \text{Var}(X) &amp;= np(1-p).
  \end{aligned}
  $$</li>
<li><strong>Poisson Distribution</strong>: Approximates binomial for large $ n $ and small $ p $, with PMF<br />
   $$
   P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!},
   \newline
   \text{mean and variance both } \lambda.
   $$</li>
<li><strong>Gaussian Distribution</strong>: Describes continuous variables from cumulative small effects, with PDF<br />
  $$
  f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}},
  \newline
  \text{mean } \mu, \quad \text{variance } \sigma^2.
  $$</li>
</ol>
</li>
</ul>
<p>Drawing on resources like MIT OCW, NPTEL, and Shreve's <em>Stochastic Calculus</em>, this exploration establishes a foundation for addressing complex problems in quantitative finance, equipping readers to approach problems like those in the Bertrand Paradox with structured probabilistic concepts.</p>
<h2 id="personal-remarks">Personal Remarks</h2>
<p>I never expected this post to take so long. I encountered numerous challenges while trying to convey what I wanted to say&mdash; although this isn't my first time typing out latex code, it was still quite a hassle. There were also various other unforeseen hinderances, that slowed this down even more. Originally I intended this post to simply be a weekend thing&mdash; but it just didn't seem to end. It has been nearly a month since the due date. (Check the Github commit history, if want to know what I'm talking about: <a href="https://github.com/kyuQee/kyuqee.github.io/commits/main/?since=2025-05-13&amp;until=2025-06-11">Github commit history</a>) <br />
In the end I had to cut out the applications, as well as the intuitive exploration of the Gaussian, which I was very much looking forward to share. However due the length of this post, as well as the time it has taken, I simply cannot continue it any further. 
I might post higher-level posts exploring actual quantitative trading stuff, but this series has to be put on hold for a while. </p>
<p>The resources I recommended however, are leagues better than this post, and worth exploring, if one has the time. I hope this post might've inspired atleast one person to dive into this world, and experience the true beauty of math, that they try to hide from us in High School.</p></div>
</article>

        </main>
    </div>
</body>

</html>